{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea0e0067",
   "metadata": {},
   "source": [
    "# Prepare the train and the test data (set parameters in cells 17 & 23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bd9c41",
   "metadata": {},
   "source": [
    "This code does the following:\n",
    "1. adds the nacis groupings \n",
    "2. adds default flags (default next year, default in 2 years, defaulted in the past, will ever default in the future)\n",
    "3. removes observations for which there was a default within the last 3 years\n",
    "4. removes NaN according to column selection\n",
    "5. logs and standardizes data\n",
    "\n",
    "Important parameters in section below:\n",
    "chosing which columns matter for NaN: \"cols_for_NaN_removal\" (cell 17)\n",
    "chosing target column (what will be your y value): \"target_col\" (cell 23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "b1ed0d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "782a9ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config:\n",
    "# Define the target industry (if any)\n",
    "target_industry = \"Food Services and Entertainment\" #if none, type None\n",
    "\n",
    "crisis_period = \"n\" #y/n (no means that the entire period is considered)\n",
    "\n",
    "target_outcome = \"default_next_2y\"#default_next_2y, default_next_1y\n",
    "\n",
    "columns_set = \"all\" #all/less\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "e62b5413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 493678 entries, 0 to 493677\n",
      "Data columns (total 98 columns):\n",
      " #   Column                                           Non-Null Count   Dtype  \n",
      "---  ------                                           --------------   -----  \n",
      " 0   gvkey                                            493678 non-null  int64  \n",
      " 1   datadate                                         493678 non-null  object \n",
      " 2   fyear                                            493678 non-null  int64  \n",
      " 3   conm                                             493678 non-null  object \n",
      " 4   tic                                              493473 non-null  object \n",
      " 5   naicsh                                           359110 non-null  float64\n",
      " 6   revt                                             397236 non-null  float64\n",
      " 7   cogs                                             356312 non-null  float64\n",
      " 8   ebit                                             353703 non-null  float64\n",
      " 9   oibdp                                            351589 non-null  float64\n",
      " 10  re                                               388408 non-null  float64\n",
      " 11  ni                                               356396 non-null  float64\n",
      " 12  xint                                             377815 non-null  float64\n",
      " 13  dp                                               385608 non-null  float64\n",
      " 14  at                                               399810 non-null  float64\n",
      " 15  act                                              334800 non-null  float64\n",
      " 16  che                                              358150 non-null  float64\n",
      " 17  rect                                             350867 non-null  float64\n",
      " 18  ap                                               369768 non-null  float64\n",
      " 19  invt                                             363054 non-null  float64\n",
      " 20  lt                                               398991 non-null  float64\n",
      " 21  lct                                              339022 non-null  float64\n",
      " 22  dlc                                              396552 non-null  float64\n",
      " 23  dltt                                             398446 non-null  float64\n",
      " 24  ceq                                              398962 non-null  float64\n",
      " 25  csho                                             420651 non-null  float64\n",
      " 26  oancf                                            294993 non-null  float64\n",
      " 27  capx                                             351120 non-null  float64\n",
      " 28  fincf                                            295055 non-null  float64\n",
      " 29  ivncf                                            295055 non-null  float64\n",
      " 30  dv                                               349289 non-null  float64\n",
      " 31  prcc_f                                           420691 non-null  float64\n",
      " 32  ajex                                             493481 non-null  float64\n",
      " 33  at_fn                                            95929 non-null   object \n",
      " 34  TL_flag                                          493678 non-null  int64  \n",
      " 35  gross_margin                                     323190 non-null  float64\n",
      " 36  net_profit_margin                                327765 non-null  float64\n",
      " 37  roa                                              355068 non-null  float64\n",
      " 38  roe                                              355279 non-null  float64\n",
      " 39  asset_turnover                                   395879 non-null  float64\n",
      " 40  cash_to_assets                                   356933 non-null  float64\n",
      " 41  fixed_asset_intensity                            333589 non-null  float64\n",
      " 42  net_working_capital_to_assets                    333243 non-null  float64\n",
      " 43  capex_to_assets                                  349920 non-null  float64\n",
      " 44  current_ratio                                    333923 non-null  float64\n",
      " 45  quick_ratio                                      336283 non-null  float64\n",
      " 46  total_debt                                       395790 non-null  float64\n",
      " 47  debt_to_assets                                   394575 non-null  float64\n",
      " 48  debt_to_equity                                   394711 non-null  float64\n",
      " 49  liabilities_to_assets                            397775 non-null  float64\n",
      " 50  interest_coverage                                296036 non-null  float64\n",
      " 51  long_term_debt_ratio                             397365 non-null  float64\n",
      " 52  book_value_per_share                             349556 non-null  float64\n",
      " 53  earnings_per_share                               348354 non-null  float64\n",
      " 54  dividend_payout_ratio                            348952 non-null  float64\n",
      " 55  dividend_yield                                   348477 non-null  float64\n",
      " 56  retention_ratio                                  348952 non-null  float64\n",
      " 57  market_capitalization                            373089 non-null  float64\n",
      " 58  free_cash_flow_to_sales                          265453 non-null  float64\n",
      " 59  financing_cash_flow_to_assets                    293891 non-null  float64\n",
      " 60  investing_cash_flow_to_assets                    293891 non-null  float64\n",
      " 61  days_sales_outstanding                           321638 non-null  float64\n",
      " 62  days_inventory_outstanding                       327035 non-null  float64\n",
      " 63  days_payables_outstanding                        329180 non-null  float64\n",
      " 64  cash_conversion_cycle                            314604 non-null  float64\n",
      " 65  revt_1_year_pct_change                           336087 non-null  float64\n",
      " 66  at_1_year_pct_change                             362926 non-null  float64\n",
      " 67  ni_1_year_pct_change                             322435 non-null  float64\n",
      " 68  oibdp_1_year_pct_change                          317381 non-null  float64\n",
      " 69  ceq_1_year_pct_change                            362791 non-null  float64\n",
      " 70  total_debt_1_year_pct_change                     305927 non-null  float64\n",
      " 71  capx_1_year_pct_change                           291999 non-null  float64\n",
      " 72  market_capitalization_1_year_pct_change          332675 non-null  float64\n",
      " 73  free_cash_flow_to_sales_1_year_pct_change        236424 non-null  float64\n",
      " 74  financing_cash_flow_to_assets_1_year_pct_change  257566 non-null  float64\n",
      " 75  cash_conversion_cycle_1_year_pct_change          281449 non-null  float64\n",
      " 76  revt_2_year_pct_change                           305469 non-null  float64\n",
      " 77  at_2_year_pct_change                             329021 non-null  float64\n",
      " 78  ni_2_year_pct_change                             291357 non-null  float64\n",
      " 79  oibdp_2_year_pct_change                          286639 non-null  float64\n",
      " 80  ceq_2_year_pct_change                            328750 non-null  float64\n",
      " 81  total_debt_2_year_pct_change                     276251 non-null  float64\n",
      " 82  capx_2_year_pct_change                           264689 non-null  float64\n",
      " 83  market_capitalization_2_year_pct_change          297184 non-null  float64\n",
      " 84  free_cash_flow_to_sales_2_year_pct_change        211581 non-null  float64\n",
      " 85  financing_cash_flow_to_assets_2_year_pct_change  231428 non-null  float64\n",
      " 86  cash_conversion_cycle_2_year_pct_change          253057 non-null  float64\n",
      " 87  revt_5_year_pct_change                           229832 non-null  float64\n",
      " 88  at_5_year_pct_change                             245630 non-null  float64\n",
      " 89  ni_5_year_pct_change                             215549 non-null  float64\n",
      " 90  oibdp_5_year_pct_change                          212168 non-null  float64\n",
      " 91  ceq_5_year_pct_change                            245215 non-null  float64\n",
      " 92  total_debt_5_year_pct_change                     204474 non-null  float64\n",
      " 93  capx_5_year_pct_change                           197615 non-null  float64\n",
      " 94  market_capitalization_5_year_pct_change          215247 non-null  float64\n",
      " 95  free_cash_flow_to_sales_5_year_pct_change        153671 non-null  float64\n",
      " 96  financing_cash_flow_to_assets_5_year_pct_change  168176 non-null  float64\n",
      " 97  cash_conversion_cycle_5_year_pct_change          186678 non-null  float64\n",
      "dtypes: float64(91), int64(3), object(4)\n",
      "memory usage: 369.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# set the path to the file obtained from cleandata.ipynb\n",
    "compustat_pre_cleaned_path = \"/Users/adamsujecki/ST310-Predicting-Company-Bankruptcy/notebooks/recent_compustat_df_1980_deduplicated_extended_winsor.csv\"\n",
    "\n",
    "compustat_pre_cleaned = pd.read_csv(compustat_pre_cleaned_path)\n",
    "\n",
    "compustat_pre_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5dc453",
   "metadata": {},
   "source": [
    "# Naics groupings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "af2fddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_naics_industry_group(df, naics_col='naicsh'):\n",
    "    \"\"\"\n",
    "    Add industry group classification based on 2-digit NAICS codes.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        Must contain the naics_col column\n",
    "    naics_col : str, default 'naicsh'\n",
    "        Column name containing NAICS codes\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame with added columns:\n",
    "        - naics_2_digit: First 2 digits of NAICS code\n",
    "        - naics_industry_group: Industry group classification string\n",
    "    \n",
    "    Industry Groups:\n",
    "    ----------------\n",
    "    - Capital Intensive: 11, 21, 22, 23, 31-33, 48-49\n",
    "    - Trade: 42, 44-45\n",
    "    - Financial and Real Estate: 52, 53\n",
    "    - Administrative: 54, 55, 56, 81, 92\n",
    "    - Healthcare and Education: 61, 62\n",
    "    - Food Services and Entertainment: 71, 72\n",
    "    - Other: All other codes\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Extract first 2 digits of NAICS code\n",
    "    # Handle float (e.g., 523100.0) by converting to int first, then string\n",
    "    df['naics_2_digit'] = (\n",
    "        df[naics_col]\n",
    "        .apply(lambda x: str(int(x))[:2] if pd.notna(x) else np.nan)\n",
    "    )\n",
    "    \n",
    "    # Convert to numeric for easier comparison\n",
    "    df['_naics_2_numeric'] = pd.to_numeric(df['naics_2_digit'], errors='coerce')\n",
    "    \n",
    "    # Define industry group mapping\n",
    "    def classify_industry(code):\n",
    "        if pd.isna(code):\n",
    "            return np.nan\n",
    "        \n",
    "        code = int(code)\n",
    "        \n",
    "        # Capital Intensive: 11, 21, 22, 23, 31-33, 48-49\n",
    "        if code in [11, 21, 22, 23] or (31 <= code <= 33) or (48 <= code <= 49):\n",
    "            return 'Capital Intensive'\n",
    "        \n",
    "        # Trade: 42, 44-45\n",
    "        elif code == 42 or (44 <= code <= 45):\n",
    "            return 'Trade'\n",
    "        \n",
    "        # Financial and Real Estate: 52, 53\n",
    "        elif code in [52, 53]:\n",
    "            return 'Financial and Real Estate'\n",
    "        \n",
    "        # Administrative: 54, 55, 56, 81, 92\n",
    "        elif code in [54, 55, 56, 81, 92]:\n",
    "            return 'Administrative'\n",
    "        \n",
    "        # Healthcare and Education: 61, 62\n",
    "        elif code in [61, 62]:\n",
    "            return 'Healthcare and Education'\n",
    "        \n",
    "        # Food Services and Entertainment: 71, 72\n",
    "        elif code in [71, 72]:\n",
    "            return 'Food Services and Entertainment'\n",
    "        \n",
    "        else:\n",
    "            return 'Other'\n",
    "    \n",
    "    df['naics_industry_group'] = df['_naics_2_numeric'].apply(classify_industry)\n",
    "    \n",
    "    # Drop helper column\n",
    "    df = df.drop(columns=['_naics_2_numeric'])\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"NAICS Industry Group Summary:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    total = len(df)\n",
    "    non_null = df['naics_industry_group'].notna().sum()\n",
    "    \n",
    "    print(f\"Total observations: {total:,}\")\n",
    "    print(f\"Observations with NAICS code: {non_null:,}\")\n",
    "    print(f\"Missing NAICS: {total - non_null:,}\")\n",
    "    print()\n",
    "    \n",
    "    group_counts = df['naics_industry_group'].value_counts(dropna=False)\n",
    "    print(\"Distribution by industry group:\")\n",
    "    for group, count in group_counts.items():\n",
    "        pct = count / total * 100\n",
    "        group_name = group if pd.notna(group) else 'Missing'\n",
    "        print(f\"  {group_name}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "387a1979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAICS Industry Group Summary:\n",
      "------------------------------------------------------------\n",
      "Total observations: 493,678\n",
      "Observations with NAICS code: 359,110\n",
      "Missing NAICS: 134,568\n",
      "\n",
      "Distribution by industry group:\n",
      "  Capital Intensive: 182,842 (37.0%)\n",
      "  Missing: 134,568 (27.3%)\n",
      "  Financial and Real Estate: 72,612 (14.7%)\n",
      "  Other: 39,719 (8.0%)\n",
      "  Trade: 25,175 (5.1%)\n",
      "  Administrative: 22,513 (4.6%)\n",
      "  Food Services and Entertainment: 8,942 (1.8%)\n",
      "  Healthcare and Education: 7,307 (1.5%)\n"
     ]
    }
   ],
   "source": [
    "compustat_pre_cleaned = add_naics_industry_group(compustat_pre_cleaned, naics_col='naicsh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3de7617",
   "metadata": {},
   "source": [
    "# Default flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "6c5ce439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_default_flags(df, default_indicator='TL_flag', default_value=1):\n",
    "    \"\"\"\n",
    "    Add multiple default-related columns to the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas DataFrame\n",
    "        Must contain columns: gvkey, fyear, and the default_indicator column\n",
    "    default_indicator : str, default 'TL_flag'\n",
    "        Column name that indicates default status (1 = default, 0 = no default)\n",
    "    default_value : int or float, default 1\n",
    "        Value in default_indicator that represents a default event\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas DataFrame with added columns:\n",
    "        - default_next_1y\n",
    "        - default_next_2y\n",
    "        - default_past_3y_plus\n",
    "        - default_ever_future\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(['gvkey', 'fyear']).reset_index(drop=True)\n",
    "\n",
    "    # ============================================================\n",
    "    # Helper: all default events\n",
    "    # ============================================================\n",
    "    defaults = (\n",
    "        df.loc[df[default_indicator] == default_value, ['gvkey', 'fyear']]\n",
    "        .rename(columns={'fyear': 'default_year'})\n",
    "    )\n",
    "\n",
    "    # ============================================================\n",
    "    # 1) DEFAULT IN NEXT 1 YEAR (fyear + 1)\n",
    "    # ============================================================\n",
    "    next_1y = (\n",
    "        df[['gvkey', 'fyear', default_indicator]]\n",
    "        .assign(fyear=lambda x: x['fyear'] - 1)\n",
    "        .rename(columns={default_indicator: 'default_next_1y'})\n",
    "    )\n",
    "\n",
    "    df = df.merge(next_1y, on=['gvkey', 'fyear'], how='left')\n",
    "\n",
    "    df['default_next_1y'] = np.where(\n",
    "        df['default_next_1y'].isna(),\n",
    "        np.nan,\n",
    "        (df['default_next_1y'] == default_value).astype(float)\n",
    "    )\n",
    "\n",
    "    # ============================================================\n",
    "    # 2) DEFAULT IN NEXT 2 YEARS (fyear + 1 OR fyear + 2)\n",
    "    # ============================================================\n",
    "    y1 = (\n",
    "        df[['gvkey', 'fyear', default_indicator]]\n",
    "        .assign(fyear=lambda x: x['fyear'] - 1)\n",
    "        .rename(columns={default_indicator: 'y1'})\n",
    "    )\n",
    "\n",
    "    y2 = (\n",
    "        df[['gvkey', 'fyear', default_indicator]]\n",
    "        .assign(fyear=lambda x: x['fyear'] - 2)\n",
    "        .rename(columns={default_indicator: 'y2'})\n",
    "    )\n",
    "\n",
    "    df = df.merge(y1, on=['gvkey', 'fyear'], how='left')\n",
    "    df = df.merge(y2, on=['gvkey', 'fyear'], how='left')\n",
    "\n",
    "    has_any_year = df[['y1', 'y2']].notna().any(axis=1)\n",
    "    has_default = (\n",
    "        (df['y1'] == default_value) |\n",
    "        (df['y2'] == default_value)\n",
    "    )\n",
    "\n",
    "    df['default_next_2y'] = np.where(\n",
    "        ~has_any_year,\n",
    "        np.nan,\n",
    "        has_default.astype(float)\n",
    "    )\n",
    "\n",
    "    df = df.drop(columns=['y1', 'y2'])\n",
    "\n",
    "    # ============================================================\n",
    "    # 3) DEFAULT MORE THAN 3 YEARS AGO (fyear - 4 or earlier)\n",
    "    # ============================================================\n",
    "    past = df.merge(defaults, on='gvkey', how='left')\n",
    "\n",
    "    past['past_default'] = (past['fyear'] - past['default_year']) > 3\n",
    "\n",
    "    past_flag = (\n",
    "        past.groupby(['gvkey', 'fyear'])['past_default']\n",
    "        .any()\n",
    "        .astype(float)\n",
    "        .reset_index(name='default_past_3y_plus')\n",
    "    )\n",
    "\n",
    "    df = df.merge(past_flag, on=['gvkey', 'fyear'], how='left')\n",
    "    df['default_past_3y_plus'] = df['default_past_3y_plus'].fillna(0.0)\n",
    "\n",
    "    # ============================================================\n",
    "    # 4) DEFAULT EVER IN FUTURE (any year > fyear)\n",
    "    # ============================================================\n",
    "    future = df[['gvkey', 'fyear']].merge(defaults, on='gvkey', how='left')\n",
    "\n",
    "    future['future_default'] = (future['default_year'] > future['fyear'])\n",
    "\n",
    "    future_flag = (\n",
    "        future.groupby(['gvkey', 'fyear'])['future_default']\n",
    "        .any()\n",
    "        .reset_index(name='default_ever_future')\n",
    "    )\n",
    "\n",
    "    df = df.merge(future_flag, on=['gvkey', 'fyear'], how='left')\n",
    "\n",
    "    # Identify whether future years exist (not just defaults)\n",
    "    max_fyear = df.groupby('gvkey')['fyear'].transform('max')\n",
    "\n",
    "    df['default_ever_future'] = np.where(\n",
    "        df['fyear'] == max_fyear,\n",
    "        np.nan,                      # no future years exist\n",
    "        df['default_ever_future'].astype(float)\n",
    "    )\n",
    "\n",
    "    # ============================================================\n",
    "    # SUMMARY (optional diagnostic output)\n",
    "    # ============================================================\n",
    "    print(\"Default Flags Summary:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for col in [\n",
    "        'default_next_1y',\n",
    "        'default_next_2y',\n",
    "        'default_past_3y_plus',\n",
    "        'default_ever_future'\n",
    "    ]:\n",
    "        non_null = df[col].notna().sum()\n",
    "        defaults_n = (df[col] == 1).sum()\n",
    "        rate = defaults_n / non_null * 100 if non_null > 0 else 0\n",
    "\n",
    "        print(f\"{col}:\")\n",
    "        print(f\"  Non-null observations: {non_null:,}\")\n",
    "        print(f\"  Defaults (=1): {defaults_n:,} ({rate:.2f}%)\\n\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "457543d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Flags Summary:\n",
      "------------------------------------------------------------\n",
      "default_next_1y:\n",
      "  Non-null observations: 448,969\n",
      "  Defaults (=1): 1,735 (0.39%)\n",
      "\n",
      "default_next_2y:\n",
      "  Non-null observations: 449,054\n",
      "  Defaults (=1): 2,623 (0.58%)\n",
      "\n",
      "default_past_3y_plus:\n",
      "  Non-null observations: 493,678\n",
      "  Defaults (=1): 3,608 (0.73%)\n",
      "\n",
      "default_ever_future:\n",
      "  Non-null observations: 449,141\n",
      "  Defaults (=1): 9,374 (2.09%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compustat_pre_cleaned = add_default_flags(compustat_pre_cleaned, default_indicator='TL_flag', default_value=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66b627c",
   "metadata": {},
   "source": [
    "# Removing periods between defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "fcf06d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_post_default_observations(df, default_indicator='TL_flag', default_value=1, lookback_years=3):\n",
    "    \"\"\"\n",
    "    Remove observations if the firm had a default in the previous N years.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        Must contain columns: gvkey, fyear, and the default_indicator column\n",
    "    default_indicator : str, default 'TL_flag'\n",
    "        Column name that indicates default status\n",
    "    default_value : int/float, default 1\n",
    "        Value in default_indicator that represents a default event\n",
    "    lookback_years : int, default 3\n",
    "        Number of years to look back for prior defaults\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame with post-default observations removed\n",
    "    \n",
    "    Notes:\n",
    "    ------\n",
    "    - For each observation, checks if the firm had a default in any of the\n",
    "      previous `lookback_years` years (fyear-1 through fyear-lookback_years)\n",
    "    - Removes the observation if any prior default is found\n",
    "    - The default year itself is NOT removed (only subsequent years)\n",
    "    - Handles gaps in data correctly (only looks at years that exist)\n",
    "    \n",
    "    Example (lookback_years=5):\n",
    "    --------\n",
    "    If firm A has:\n",
    "        fyear=2018, TL_flag=0  -> KEEP (no prior default)\n",
    "        fyear=2019, TL_flag=1  -> KEEP (this is the default year)\n",
    "        fyear=2020, TL_flag=0  -> REMOVE (default in 2019, within 5 years)\n",
    "        fyear=2021, TL_flag=0  -> REMOVE (default in 2019, within 5 years)\n",
    "        fyear=2022, TL_flag=0  -> REMOVE (default in 2019, within 5 years)\n",
    "        fyear=2023, TL_flag=0  -> REMOVE (default in 2019, within 5 years)\n",
    "        fyear=2024, TL_flag=0  -> REMOVE (default in 2019, within 5 years)\n",
    "        fyear=2025, TL_flag=0  -> KEEP (2019 is now 6 years ago)\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Sort by company and fiscal year\n",
    "    df = df.sort_values(['gvkey', 'fyear']).reset_index(drop=True)\n",
    "    \n",
    "    # Get all default events\n",
    "    defaults = df[df[default_indicator] == default_value][['gvkey', 'fyear']].copy()\n",
    "    defaults = defaults.rename(columns={'fyear': 'default_year'})\n",
    "    \n",
    "    # For each observation, check if there's a default in the lookback window\n",
    "    # Merge all defaults for each company\n",
    "    df_with_defaults = df.merge(defaults, on='gvkey', how='left')\n",
    "    \n",
    "    # Calculate years since default\n",
    "    df_with_defaults['years_since_default'] = df_with_defaults['fyear'] - df_with_defaults['default_year']\n",
    "    \n",
    "    # Flag observations where a default occurred in the lookback window\n",
    "    # (years_since_default between 1 and lookback_years, inclusive)\n",
    "    df_with_defaults['has_recent_default'] = (\n",
    "        (df_with_defaults['years_since_default'] >= 1) & \n",
    "        (df_with_defaults['years_since_default'] <= lookback_years)\n",
    "    )\n",
    "    \n",
    "    # Group by original observation and check if ANY prior default exists\n",
    "    recent_default_flag = df_with_defaults.groupby(['gvkey', 'fyear'])['has_recent_default'].any().reset_index()\n",
    "    recent_default_flag = recent_default_flag.rename(columns={'has_recent_default': 'remove_flag'})\n",
    "    \n",
    "    # Merge back to original dataframe\n",
    "    df = df.merge(recent_default_flag, on=['gvkey', 'fyear'], how='left')\n",
    "    \n",
    "    # Fill NaN (firms with no defaults at all) with False\n",
    "    df['remove_flag'] = df['remove_flag'].fillna(False)\n",
    "    \n",
    "    # Print summary before removal\n",
    "    total_obs = len(df)\n",
    "    obs_to_remove = df['remove_flag'].sum()\n",
    "    firms_affected = df[df['remove_flag']]['gvkey'].nunique()\n",
    "    \n",
    "    print(\"Post-Default Observation Removal Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Lookback window: {lookback_years} years\")\n",
    "    print(f\"Total observations before: {total_obs:,}\")\n",
    "    print(f\"Observations to remove: {obs_to_remove:,}\")\n",
    "    print(f\"Firms affected: {firms_affected:,}\")\n",
    "    print(f\"Observations remaining: {total_obs - obs_to_remove:,}\")\n",
    "    print(f\"Percentage removed: {obs_to_remove / total_obs * 100:.2f}%\")\n",
    "    \n",
    "    # Remove flagged observations\n",
    "    df_clean = df[~df['remove_flag']].drop(columns=['remove_flag']).reset_index(drop=True)\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "c25a6d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-Default Observation Removal Summary:\n",
      "--------------------------------------------------\n",
      "Lookback window: 3 years\n",
      "Total observations before: 493,678\n",
      "Observations to remove: 2,353\n",
      "Firms affected: 729\n",
      "Observations remaining: 491,325\n",
      "Percentage removed: 0.48%\n"
     ]
    }
   ],
   "source": [
    "compustat_pre_cleaned = remove_post_default_observations(compustat_pre_cleaned, default_indicator='TL_flag', default_value=1, lookback_years=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f10dd18",
   "metadata": {},
   "source": [
    "# Set the crisis period parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "cd190311",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Responsible for making the crisis period:\n",
    "def filter_crisis_years(df, year_col='fyear'):\n",
    "    crisis_years = (\n",
    "        list(range(1981, 1984)) +\n",
    "        list(range(1989, 1992)) +\n",
    "        list(range(2000, 2003)) +\n",
    "        list(range(2006, 2011)) +\n",
    "        list(range(2019, 2023))\n",
    "    )\n",
    "    return df[df[year_col].isin(crisis_years)]\n",
    "\n",
    "if crisis_period == \"y\":\n",
    "    compustat_pre_cleaned = filter_crisis_years(compustat_pre_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "0862db05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gvkey', 'datadate', 'fyear', 'conm', 'tic', 'naicsh', 'revt', 'cogs', 'ebit', 'oibdp', 're', 'ni', 'xint', 'dp', 'at', 'act', 'che', 'rect', 'ap', 'invt', 'lt', 'lct', 'dlc', 'dltt', 'ceq', 'csho', 'oancf', 'capx', 'fincf', 'ivncf', 'dv', 'prcc_f', 'ajex', 'at_fn', 'TL_flag', 'gross_margin', 'net_profit_margin', 'roa', 'roe', 'asset_turnover', 'cash_to_assets', 'fixed_asset_intensity', 'net_working_capital_to_assets', 'capex_to_assets', 'current_ratio', 'quick_ratio', 'total_debt', 'debt_to_assets', 'debt_to_equity', 'liabilities_to_assets', 'interest_coverage', 'long_term_debt_ratio', 'book_value_per_share', 'dividend_payout_ratio', 'dividend_yield', 'retention_ratio', 'market_capitalization', 'free_cash_flow_to_sales', 'financing_cash_flow_to_assets', 'investing_cash_flow_to_assets', 'days_sales_outstanding', 'days_inventory_outstanding', 'days_payables_outstanding', 'cash_conversion_cycle', 'revt_1_year_pct_change', 'at_1_year_pct_change', 'ni_1_year_pct_change', 'oibdp_1_year_pct_change', 'ceq_1_year_pct_change', 'total_debt_1_year_pct_change', 'capx_1_year_pct_change', 'market_capitalization_1_year_pct_change', 'free_cash_flow_to_sales_1_year_pct_change', 'financing_cash_flow_to_assets_1_year_pct_change', 'cash_conversion_cycle_1_year_pct_change', 'revt_2_year_pct_change', 'at_2_year_pct_change', 'ni_2_year_pct_change', 'oibdp_2_year_pct_change', 'ceq_2_year_pct_change', 'total_debt_2_year_pct_change', 'capx_2_year_pct_change', 'market_capitalization_2_year_pct_change', 'free_cash_flow_to_sales_2_year_pct_change', 'financing_cash_flow_to_assets_2_year_pct_change', 'cash_conversion_cycle_2_year_pct_change', 'revt_5_year_pct_change', 'at_5_year_pct_change', 'ni_5_year_pct_change', 'oibdp_5_year_pct_change', 'ceq_5_year_pct_change', 'total_debt_5_year_pct_change', 'capx_5_year_pct_change', 'market_capitalization_5_year_pct_change', 'free_cash_flow_to_sales_5_year_pct_change', 'financing_cash_flow_to_assets_5_year_pct_change', 'cash_conversion_cycle_5_year_pct_change', 'naics_2_digit', 'naics_industry_group', 'default_next_1y', 'default_next_2y', 'default_past_3y_plus', 'default_ever_future']\n"
     ]
    }
   ],
   "source": [
    "compustat_pre_cleaned = compustat_pre_cleaned.drop(columns = 'earnings_per_share')\n",
    "col_list = compustat_pre_cleaned.columns.tolist()\n",
    "print(col_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e1f8f6",
   "metadata": {},
   "source": [
    "# Pick industry (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "c0dae7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan, 'Food Services and Entertainment', 'Trade', 'Capital Intensive', 'Other', 'Financial and Real Estate', 'Administrative', 'Healthcare and Education']\n"
     ]
    }
   ],
   "source": [
    "print(compustat_pre_cleaned['naics_industry_group'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "c2abed49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_industry(df, industry_column, target_industry):\n",
    "    df_copy=df.copy()\n",
    "    return df_copy[df_copy[industry_column] == target_industry]\n",
    "\n",
    "if target_industry:\n",
    "    compustat_pre_cleaned = filter_by_industry(compustat_pre_cleaned, \"naics_industry_group\", target_industry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc2e882",
   "metadata": {},
   "source": [
    "# Column set configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "a78454ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_removal = 'default_next_1y' if target_outcome == 'default_next_2y' else 'default_next_2y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "ae250050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['revt', 'cogs', 'ebit', 'oibdp', 're', 'ni', 'xint', 'dp', 'at', 'act', 'che', 'rect', 'ap', 'invt', 'lt', 'lct', 'dlc', 'dltt', 'ceq', 'oancf', 'capx', 'fincf', 'ivncf', 'dv', 'prcc_f', 'gross_margin', 'net_profit_margin', 'roa', 'roe', 'asset_turnover', 'cash_to_assets', 'fixed_asset_intensity', 'net_working_capital_to_assets', 'capex_to_assets', 'current_ratio', 'quick_ratio', 'total_debt', 'debt_to_assets', 'debt_to_equity', 'liabilities_to_assets', 'interest_coverage', 'long_term_debt_ratio', 'book_value_per_share', 'dividend_payout_ratio', 'dividend_yield', 'retention_ratio', 'market_capitalization', 'free_cash_flow_to_sales', 'financing_cash_flow_to_assets', 'investing_cash_flow_to_assets', 'days_sales_outstanding', 'days_inventory_outstanding', 'days_payables_outstanding', 'cash_conversion_cycle', 'revt_1_year_pct_change', 'at_1_year_pct_change', 'ni_1_year_pct_change', 'oibdp_1_year_pct_change', 'ceq_1_year_pct_change', 'total_debt_1_year_pct_change', 'capx_1_year_pct_change', 'market_capitalization_1_year_pct_change', 'free_cash_flow_to_sales_1_year_pct_change', 'financing_cash_flow_to_assets_1_year_pct_change', 'cash_conversion_cycle_1_year_pct_change', 'revt_2_year_pct_change', 'at_2_year_pct_change', 'ni_2_year_pct_change', 'oibdp_2_year_pct_change', 'ceq_2_year_pct_change', 'total_debt_2_year_pct_change', 'capx_2_year_pct_change', 'market_capitalization_2_year_pct_change', 'free_cash_flow_to_sales_2_year_pct_change', 'financing_cash_flow_to_assets_2_year_pct_change', 'cash_conversion_cycle_2_year_pct_change', 'revt_5_year_pct_change', 'at_5_year_pct_change', 'ni_5_year_pct_change', 'oibdp_5_year_pct_change', 'ceq_5_year_pct_change', 'total_debt_5_year_pct_change', 'capx_5_year_pct_change', 'market_capitalization_5_year_pct_change', 'free_cash_flow_to_sales_5_year_pct_change', 'financing_cash_flow_to_assets_5_year_pct_change', 'cash_conversion_cycle_5_year_pct_change', 'default_next_2y']\n"
     ]
    }
   ],
   "source": [
    "if columns_set == 'all':\n",
    "\n",
    "    compustat_pre_cleaned.drop(columns=['gvkey', 'datadate', 'fyear', 'conm', 'tic', 'naicsh','ajex','csho','at_fn','TL_flag'], inplace=True)\n",
    "    compustat_pre_cleaned.drop(columns=['naics_2_digit','naics_industry_group',target_removal,'default_past_3y_plus','default_ever_future'], inplace=True)\n",
    "\n",
    "    col_list = compustat_pre_cleaned.columns.tolist()\n",
    "    print(col_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "21b29767",
   "metadata": {},
   "outputs": [],
   "source": [
    "if columns_set == 'less':\n",
    "    compustat_pre_cleaned = compustat_pre_cleaned[['revt', 'oibdp', 'ni', 'xint', 'dp', 'at', 'act', 'che', \n",
    "        'lt', 'lct', 'dlc', 'dltt', 'ceq', 'dv', \n",
    "        'cogs','ebit','re','rect','invt','oancf', 'capx', 'fincf', 'ivncf','prcc_f', 'ap', \n",
    "        'gross_margin', 'net_profit_margin', 'roa', 'roe', 'asset_turnover','cash_to_assets', 'fixed_asset_intensity', 'current_ratio', 'quick_ratio','total_debt', 'debt_to_assets', 'debt_to_equity', 'liabilities_to_assets','interest_coverage', 'long_term_debt_ratio', 'dividend_payout_ratio', 'dividend_yield', 'retention_ratio','revt_1_year_pct_change', 'at_1_year_pct_change', 'ni_1_year_pct_change','oibdp_1_year_pct_change', 'ceq_1_year_pct_change', 'total_debt_1_year_pct_change','revt_2_year_pct_change', 'at_2_year_pct_change', 'ni_2_year_pct_change','oibdp_2_year_pct_change', 'ceq_2_year_pct_change', 'total_debt_2_year_pct_change', 'revt_5_year_pct_change', 'at_5_year_pct_change', 'ni_5_year_pct_change','oibdp_5_year_pct_change', 'ceq_5_year_pct_change', 'total_debt_5_year_pct_change', \n",
    "        'capex_to_assets','investing_cash_flow_to_assets','days_sales_outstanding','days_inventory_outstanding','net_working_capital_to_assets','free_cash_flow_to_sales','financing_cash_flow_to_assets','market_capitalization', 'days_payables_outstanding','cash_conversion_cycle','market_capitalization_1_year_pct_change', \n",
    "        'market_capitalization_2_year_pct_change', 'book_value_per_share',target_outcome ]]\n",
    "    col_list = compustat_pre_cleaned.columns.tolist()\n",
    "    print(col_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06b00cc",
   "metadata": {},
   "source": [
    "# Remove NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a89730",
   "metadata": {},
   "source": [
    "Important: in cell 17, define which columns matter for you when removing NaN values. For example, if you do not care about missing values in columns like \"revt_5_year_pct_change\", do not include it in the list. \n",
    "\n",
    "The function remove_nan_rows takes in a list of columns as an argument, and removes all rows, which have a NaN entry in at least one of the specified columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "a3738c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan_rows(df, cols, how='any'):\n",
    "    \"\"\"\n",
    "    Remove rows containing NaN in specified columns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The dataset\n",
    "    cols : list\n",
    "        List of column names to check for NaN\n",
    "    how : str, default 'any'\n",
    "        - 'any': Remove row if ANY of the specified columns has NaN\n",
    "        - 'all': Remove row only if ALL of the specified columns have NaN\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame with rows removed\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    # Remove rows where any of these columns is NaN\n",
    "    df_clean = remove_nan_rows(df, cols=['revt', 'at', 'ni'], how='any')\n",
    "    \n",
    "    # Remove rows only where ALL of these columns are NaN\n",
    "    df_clean = remove_nan_rows(df, cols=['revt', 'at', 'ni'], how='all')\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter to columns that exist\n",
    "    cols_exist = [c for c in cols if c in df.columns]\n",
    "    cols_missing = [c for c in cols if c not in df.columns]\n",
    "    \n",
    "    if cols_missing:\n",
    "        print(f\"Warning: Columns not found (ignored): {cols_missing}\")\n",
    "    \n",
    "    if not cols_exist:\n",
    "        print(\"No valid columns specified. Returning original DataFrame.\")\n",
    "        return df\n",
    "    \n",
    "    # Count NaN before removal\n",
    "    rows_before = len(df)\n",
    "    \n",
    "    # Per-column NaN counts\n",
    "    print(\"NaN counts per column before removal:\")\n",
    "    print(\"-\" * 40)\n",
    "    for col in cols_exist:\n",
    "        nan_count = df[col].isna().sum()\n",
    "        pct = nan_count / rows_before * 100\n",
    "        print(f\"  {col}: {nan_count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Remove rows\n",
    "    df_clean = df.dropna(subset=cols_exist, how=how).reset_index(drop=True)\n",
    "    \n",
    "    rows_after = len(df_clean)\n",
    "    rows_removed = rows_before - rows_after\n",
    "    \n",
    "    # Summary\n",
    "    print()\n",
    "    print(f\"Removal method: '{how}'\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Rows before: {rows_before:,}\")\n",
    "    print(f\"Rows removed: {rows_removed:,} ({rows_removed / rows_before * 100:.1f}%)\")\n",
    "    print(f\"Rows after: {rows_after:,}\")\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "40a6168a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN counts per column before removal:\n",
      "----------------------------------------\n",
      "  revt: 274 (3.1%)\n",
      "  oibdp: 321 (3.6%)\n",
      "  ni: 302 (3.4%)\n",
      "  xint: 479 (5.4%)\n",
      "  dp: 293 (3.3%)\n",
      "  at: 215 (2.4%)\n",
      "  act: 547 (6.2%)\n",
      "  che: 251 (2.8%)\n",
      "  lt: 227 (2.6%)\n",
      "  lct: 485 (5.5%)\n",
      "  dlc: 231 (2.6%)\n",
      "  dltt: 231 (2.6%)\n",
      "  ceq: 228 (2.6%)\n",
      "  dv: 437 (5.0%)\n",
      "  cogs: 303 (3.4%)\n",
      "  ebit: 303 (3.4%)\n",
      "  re: 624 (7.1%)\n",
      "  rect: 335 (3.8%)\n",
      "  invt: 339 (3.8%)\n",
      "  oancf: 1,038 (11.8%)\n",
      "  capx: 435 (4.9%)\n",
      "  fincf: 1,037 (11.7%)\n",
      "  ivncf: 1,038 (11.8%)\n",
      "  prcc_f: 1,651 (18.7%)\n",
      "  ap: 227 (2.6%)\n",
      "  gross_margin: 501 (5.7%)\n",
      "  net_profit_margin: 485 (5.5%)\n",
      "  roa: 327 (3.7%)\n",
      "  roe: 323 (3.7%)\n",
      "  asset_turnover: 299 (3.4%)\n",
      "  cash_to_assets: 272 (3.1%)\n",
      "  fixed_asset_intensity: 568 (6.4%)\n",
      "  current_ratio: 564 (6.4%)\n",
      "  quick_ratio: 509 (5.8%)\n",
      "  total_debt: 244 (2.8%)\n",
      "  debt_to_assets: 265 (3.0%)\n",
      "  debt_to_equity: 259 (2.9%)\n",
      "  liabilities_to_assets: 248 (2.8%)\n",
      "  interest_coverage: 873 (9.9%)\n",
      "  long_term_debt_ratio: 247 (2.8%)\n",
      "  dividend_payout_ratio: 439 (5.0%)\n",
      "  dividend_yield: 452 (5.1%)\n",
      "  retention_ratio: 439 (5.0%)\n",
      "  revt_1_year_pct_change: 1,075 (12.2%)\n",
      "  at_1_year_pct_change: 852 (9.7%)\n",
      "  ni_1_year_pct_change: 940 (10.6%)\n",
      "  oibdp_1_year_pct_change: 977 (11.1%)\n",
      "  ceq_1_year_pct_change: 853 (9.7%)\n",
      "  total_debt_1_year_pct_change: 1,490 (16.9%)\n",
      "  revt_2_year_pct_change: 1,679 (19.0%)\n",
      "  at_2_year_pct_change: 1,478 (16.7%)\n",
      "  ni_2_year_pct_change: 1,561 (17.7%)\n",
      "  oibdp_2_year_pct_change: 1,601 (18.1%)\n",
      "  ceq_2_year_pct_change: 1,483 (16.8%)\n",
      "  total_debt_2_year_pct_change: 2,075 (23.5%)\n",
      "  revt_5_year_pct_change: 3,231 (36.6%)\n",
      "  at_5_year_pct_change: 3,105 (35.2%)\n",
      "  ni_5_year_pct_change: 3,172 (35.9%)\n",
      "  oibdp_5_year_pct_change: 3,209 (36.4%)\n",
      "  ceq_5_year_pct_change: 3,111 (35.2%)\n",
      "  total_debt_5_year_pct_change: 3,576 (40.5%)\n",
      "  capex_to_assets: 456 (5.2%)\n",
      "  investing_cash_flow_to_assets: 1,059 (12.0%)\n",
      "  days_sales_outstanding: 580 (6.6%)\n",
      "  days_inventory_outstanding: 610 (6.9%)\n",
      "  net_working_capital_to_assets: 575 (6.5%)\n",
      "  free_cash_flow_to_sales: 1,306 (14.8%)\n",
      "  financing_cash_flow_to_assets: 1,058 (12.0%)\n",
      "  market_capitalization: 1,743 (19.7%)\n",
      "  days_payables_outstanding: 498 (5.6%)\n",
      "  cash_conversion_cycle: 711 (8.1%)\n",
      "  capx_1_year_pct_change: 1,345 (15.2%)\n",
      "  market_capitalization_1_year_pct_change: 2,352 (26.6%)\n",
      "  free_cash_flow_to_sales_1_year_pct_change: 2,125 (24.1%)\n",
      "  financing_cash_flow_to_assets_1_year_pct_change: 1,933 (21.9%)\n",
      "  capx_2_year_pct_change: 1,958 (22.2%)\n",
      "  market_capitalization_2_year_pct_change: 2,928 (33.2%)\n",
      "  free_cash_flow_to_sales_2_year_pct_change: 2,847 (32.3%)\n",
      "  financing_cash_flow_to_assets_2_year_pct_change: 2,644 (30.0%)\n",
      "  capx_5_year_pct_change: 3,476 (39.4%)\n",
      "  market_capitalization_5_year_pct_change: 4,343 (49.2%)\n",
      "  free_cash_flow_to_sales_5_year_pct_change: 4,548 (51.5%)\n",
      "  financing_cash_flow_to_assets_5_year_pct_change: 4,377 (49.6%)\n",
      "  cash_conversion_cycle_1_year_pct_change: 1,415 (16.0%)\n",
      "  cash_conversion_cycle_2_year_pct_change: 2,045 (23.2%)\n",
      "  cash_conversion_cycle_5_year_pct_change: 3,607 (40.9%)\n",
      "  book_value_per_share: 632 (7.2%)\n",
      "  default_next_2y: 797 (9.0%)\n",
      "\n",
      "Removal method: 'any'\n",
      "----------------------------------------\n",
      "Rows before: 8,827\n",
      "Rows removed: 6,441 (73.0%)\n",
      "Rows after: 2,386\n"
     ]
    }
   ],
   "source": [
    "if columns_set == 'all':\n",
    "\n",
    "    cols_for_NaN_removal = ['revt', 'oibdp', 'ni', 'xint', 'dp', 'at', 'act', 'che', 'lt', 'lct', 'dlc', 'dltt', 'ceq', 'dv', \n",
    "        'cogs','ebit','re','rect','invt','oancf', 'capx', 'fincf', 'ivncf','prcc_f', 'ap', \n",
    "        'gross_margin', 'net_profit_margin', 'roa', 'roe', 'asset_turnover','cash_to_assets', 'fixed_asset_intensity', 'current_ratio', 'quick_ratio','total_debt', 'debt_to_assets', 'debt_to_equity', 'liabilities_to_assets','interest_coverage', 'long_term_debt_ratio', 'dividend_payout_ratio', 'dividend_yield', 'retention_ratio','revt_1_year_pct_change', 'at_1_year_pct_change', 'ni_1_year_pct_change','oibdp_1_year_pct_change', 'ceq_1_year_pct_change', 'total_debt_1_year_pct_change','revt_2_year_pct_change', 'at_2_year_pct_change', 'ni_2_year_pct_change','oibdp_2_year_pct_change', 'ceq_2_year_pct_change', 'total_debt_2_year_pct_change', 'revt_5_year_pct_change', 'at_5_year_pct_change', 'ni_5_year_pct_change','oibdp_5_year_pct_change', 'ceq_5_year_pct_change', 'total_debt_5_year_pct_change', \n",
    "        'capex_to_assets','investing_cash_flow_to_assets','days_sales_outstanding','days_inventory_outstanding','net_working_capital_to_assets','free_cash_flow_to_sales','financing_cash_flow_to_assets','market_capitalization', 'days_payables_outstanding','cash_conversion_cycle', \n",
    "        'capx_1_year_pct_change','market_capitalization_1_year_pct_change','free_cash_flow_to_sales_1_year_pct_change','financing_cash_flow_to_assets_1_year_pct_change', \n",
    "        'capx_2_year_pct_change','market_capitalization_2_year_pct_change','free_cash_flow_to_sales_2_year_pct_change','financing_cash_flow_to_assets_2_year_pct_change', \n",
    "        'capx_5_year_pct_change','market_capitalization_5_year_pct_change','free_cash_flow_to_sales_5_year_pct_change','financing_cash_flow_to_assets_5_year_pct_change', \n",
    "        'cash_conversion_cycle_1_year_pct_change','cash_conversion_cycle_2_year_pct_change','cash_conversion_cycle_5_year_pct_change', 'book_value_per_share' ] # make the choice of columns you want to use for NaN removal\n",
    "\n",
    "    target_col = target_outcome\n",
    "\n",
    "    cols_for_NaN_removal.append(target_col)\n",
    "\n",
    "    compustat_pre_cleaned = remove_nan_rows(\n",
    "        compustat_pre_cleaned,\n",
    "        cols_for_NaN_removal\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "ab65b916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Longer set\n",
    "if columns_set == 'less':\n",
    "\n",
    "\n",
    "    cols_for_NaN_removal = ['revt', 'oibdp', 'ni', 'xint', 'dp', 'at', 'act', 'che', \n",
    "        'lt', 'lct', 'dlc', 'dltt', 'ceq', 'dv', \n",
    "        'cogs','ebit','re','rect','invt','oancf', 'capx', 'fincf', 'ivncf','prcc_f', 'ap', \n",
    "        'gross_margin', 'net_profit_margin', 'roa', 'roe', 'asset_turnover','cash_to_assets', 'fixed_asset_intensity', 'current_ratio', 'quick_ratio','total_debt', 'debt_to_assets', 'debt_to_equity', 'liabilities_to_assets','interest_coverage', 'long_term_debt_ratio', 'dividend_payout_ratio', 'dividend_yield', 'retention_ratio','revt_1_year_pct_change', 'at_1_year_pct_change', 'ni_1_year_pct_change','oibdp_1_year_pct_change', 'ceq_1_year_pct_change', 'total_debt_1_year_pct_change','revt_2_year_pct_change', 'at_2_year_pct_change', 'ni_2_year_pct_change','oibdp_2_year_pct_change', 'ceq_2_year_pct_change', 'total_debt_2_year_pct_change', 'revt_5_year_pct_change', 'at_5_year_pct_change', 'ni_5_year_pct_change','oibdp_5_year_pct_change', 'ceq_5_year_pct_change', 'total_debt_5_year_pct_change', \n",
    "        'capex_to_assets','investing_cash_flow_to_assets','days_sales_outstanding','days_inventory_outstanding','net_working_capital_to_assets','free_cash_flow_to_sales','financing_cash_flow_to_assets','market_capitalization', 'days_payables_outstanding','cash_conversion_cycle','market_capitalization_1_year_pct_change', \n",
    "        'market_capitalization_2_year_pct_change', 'book_value_per_share' ] # make the choice of columns you want to use for NaN removal\n",
    "\n",
    "    target_col = target_outcome\n",
    "\n",
    "    cols_for_NaN_removal.append(target_col)\n",
    "\n",
    "    compustat_pre_cleaned = remove_nan_rows(\n",
    "        compustat_pre_cleaned,\n",
    "        cols_for_NaN_removal\n",
    "        )   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c01201c",
   "metadata": {},
   "source": [
    "# Logs, standardize, and train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafc7186",
   "metadata": {},
   "source": [
    "important: check which variables are standardized/logged in cell 21, and let me know if you think something should be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "ebcdae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signed_log_transform(df, cols):\n",
    "    \"\"\"Apply signed log transformation: sign(x) * log(1 + |x|)\"\"\"\n",
    "    df = df.copy()\n",
    "    cols_exist = [c for c in cols if c in df.columns]\n",
    "    for col in cols_exist:\n",
    "        df[col] = np.sign(df[col]) * np.log1p(np.abs(df[col]))\n",
    "    return df\n",
    "\n",
    "\n",
    "def standardize(df, cols, scaling_params=None):\n",
    "    \"\"\"\n",
    "    Standardize columns. If scaling_params provided, use those (for test data).\n",
    "    Otherwise compute from data (for training data) and return params.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    cols_exist = [c for c in cols if c in df.columns]\n",
    "    \n",
    "    if scaling_params is None:\n",
    "        # Training mode: compute parameters\n",
    "        scaling_params = {}\n",
    "        for col in cols_exist:\n",
    "            mean, std = df[col].mean(), df[col].std()\n",
    "            std = std if std != 0 else 1\n",
    "            df[col] = (df[col] - mean) / std\n",
    "            scaling_params[col] = {'mean': mean, 'std': std}\n",
    "        return df, scaling_params\n",
    "    else:\n",
    "        # Test mode: use provided parameters\n",
    "        for col in cols_exist:\n",
    "            if col in scaling_params:\n",
    "                mean = scaling_params[col]['mean']\n",
    "                std = scaling_params[col]['std']\n",
    "                df[col] = (df[col] - mean) / std\n",
    "        return df\n",
    "\n",
    "\n",
    "def prepare_data(df, feature_cols, target_col, log_cols, standardize_cols,\n",
    "                 test_size=0.2, random_state=12):\n",
    "    \"\"\"\n",
    "    Prepare data for SVM training.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "    feature_cols : list\n",
    "        All feature column names\n",
    "    target_col : str\n",
    "        Target variable name (e.g., 'default_next_period')\n",
    "    log_cols : list\n",
    "        Columns to apply signed log transform\n",
    "    standardize_cols : list\n",
    "        Columns to standardize\n",
    "    test_size : float\n",
    "        Proportion of data for test set\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (train_df, test_df, scaling_params)\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Split into train/test\n",
    "    train_df, test_df = train_test_split(\n",
    "        df, test_size=test_size, random_state=random_state,\n",
    "        stratify=df[target_col]\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {len(train_df):,}\")\n",
    "    print(f\"Test set: {len(test_df):,}\")\n",
    "    print(f\"Default rate (train): {train_df[target_col].mean():.2%}\")\n",
    "    print(f\"Default rate (test): {test_df[target_col].mean():.2%}\")\n",
    "    \n",
    "    # Apply log transform\n",
    "    train_df = signed_log_transform(train_df, log_cols)\n",
    "    test_df = signed_log_transform(test_df, log_cols)\n",
    "    \n",
    "    # Standardize (fit on train, apply to test)\n",
    "    train_df, scaling_params = standardize(train_df, standardize_cols)\n",
    "    test_df = standardize(test_df, standardize_cols, scaling_params)\n",
    "    \n",
    "    return train_df, test_df, scaling_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "6b31aaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if columns_set == 'all':\n",
    "    \n",
    "    raw_level_cols = ['revt', 'oibdp', 'ni', 'xint', 'dp', 'at', 'act', 'che',\n",
    "                'lt', 'lct', 'dlc', 'dltt', 'ceq', 'dv', 'cogs','ebit','re','rect','invt','oancf', \n",
    "                'capx', 'fincf', 'ivncf','prcc_f', 'ap','total_debt','market_capitalization']\n",
    "\n",
    "    all_cols =  ['revt', 'oibdp', 'ni', 'xint', 'dp', 'at', 'act', 'che', 'lt', 'lct', 'dlc', 'dltt', 'ceq', 'dv', \n",
    "        'cogs','ebit','re','rect','invt','oancf', 'capx', 'fincf', 'ivncf','prcc_f', 'ap', \n",
    "        'gross_margin', 'net_profit_margin', 'roa', 'roe', 'asset_turnover','cash_to_assets', 'fixed_asset_intensity', 'current_ratio', 'quick_ratio','total_debt', 'debt_to_assets', 'debt_to_equity', 'liabilities_to_assets','interest_coverage', 'long_term_debt_ratio', 'dividend_payout_ratio', 'dividend_yield', 'retention_ratio','revt_1_year_pct_change', 'at_1_year_pct_change', 'ni_1_year_pct_change','oibdp_1_year_pct_change', 'ceq_1_year_pct_change', 'total_debt_1_year_pct_change','revt_2_year_pct_change', 'at_2_year_pct_change', 'ni_2_year_pct_change','oibdp_2_year_pct_change', 'ceq_2_year_pct_change', 'total_debt_2_year_pct_change', 'revt_5_year_pct_change', 'at_5_year_pct_change', 'ni_5_year_pct_change','oibdp_5_year_pct_change', 'ceq_5_year_pct_change', 'total_debt_5_year_pct_change', \n",
    "        'capex_to_assets','investing_cash_flow_to_assets','days_sales_outstanding','days_inventory_outstanding','net_working_capital_to_assets','free_cash_flow_to_sales','financing_cash_flow_to_assets','market_capitalization', 'days_payables_outstanding','cash_conversion_cycle', \n",
    "        'capx_1_year_pct_change','market_capitalization_1_year_pct_change','free_cash_flow_to_sales_1_year_pct_change','financing_cash_flow_to_assets_1_year_pct_change', \n",
    "        'capx_2_year_pct_change','market_capitalization_2_year_pct_change','free_cash_flow_to_sales_2_year_pct_change','financing_cash_flow_to_assets_2_year_pct_change', \n",
    "        'capx_5_year_pct_change','market_capitalization_5_year_pct_change','free_cash_flow_to_sales_5_year_pct_change','financing_cash_flow_to_assets_5_year_pct_change', \n",
    "        'cash_conversion_cycle_1_year_pct_change','cash_conversion_cycle_2_year_pct_change','cash_conversion_cycle_5_year_pct_change' ,'book_value_per_share']\n",
    "\n",
    "\n",
    "\n",
    "    log_cols = raw_level_cols  # Only log transform raw levels\n",
    "    feature_cols = all_cols\n",
    "\n",
    "    standardize_cols = feature_cols  # Standardize all features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "1d06c722",
   "metadata": {},
   "outputs": [],
   "source": [
    "if columns_set == 'less':       \n",
    "# Less features set:\n",
    "\n",
    "        raw_level_cols = ['revt', 'oibdp', 'ni', 'xint', 'dp', 'at', 'act', 'che', \n",
    "        'lt', 'lct', 'dlc', 'dltt', 'ceq', 'dv', \n",
    "        'cogs','ebit','re','rect','invt','oancf', \n",
    "        'capx', 'fincf', 'ivncf','prcc_f', 'ap','total_debt','market_capitalization']\n",
    "\n",
    "        all_cols =  ['revt', 'oibdp', 'ni', 'xint', 'dp', 'at', 'act', 'che', \n",
    "        'lt', 'lct', 'dlc', 'dltt', 'ceq', 'dv', \n",
    "        'cogs','ebit','re','rect','invt','oancf', 'capx', 'fincf', 'ivncf','prcc_f', 'ap', \n",
    "        'gross_margin', 'net_profit_margin', 'roa', 'roe', 'asset_turnover','cash_to_assets', 'fixed_asset_intensity', 'current_ratio', 'quick_ratio','total_debt', 'debt_to_assets', 'debt_to_equity', 'liabilities_to_assets','interest_coverage', 'long_term_debt_ratio', 'dividend_payout_ratio', 'dividend_yield', 'retention_ratio','revt_1_year_pct_change', 'at_1_year_pct_change', 'ni_1_year_pct_change','oibdp_1_year_pct_change', 'ceq_1_year_pct_change', 'total_debt_1_year_pct_change','revt_2_year_pct_change', 'at_2_year_pct_change', 'ni_2_year_pct_change','oibdp_2_year_pct_change', 'ceq_2_year_pct_change', 'total_debt_2_year_pct_change', 'revt_5_year_pct_change', 'at_5_year_pct_change', 'ni_5_year_pct_change','oibdp_5_year_pct_change', 'ceq_5_year_pct_change', 'total_debt_5_year_pct_change', \n",
    "        'capex_to_assets','investing_cash_flow_to_assets','days_sales_outstanding','days_inventory_outstanding','net_working_capital_to_assets','free_cash_flow_to_sales','financing_cash_flow_to_assets','market_capitalization', 'days_payables_outstanding','cash_conversion_cycle','market_capitalization_1_year_pct_change', \n",
    "        'market_capitalization_2_year_pct_change', 'book_value_per_share' ]\n",
    "\n",
    "        log_cols = raw_level_cols  # Only log transform raw levels\n",
    "        feature_cols = all_cols\n",
    "\n",
    "        standardize_cols = feature_cols  # Standardize all features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27699811",
   "metadata": {},
   "source": [
    "chose what is your target columns (is it default in current period, in the t+1, t+2, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "44739902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 1,908\n",
      "Test set: 478\n",
      "Default rate (train): 1.21%\n",
      "Default rate (test): 1.26%\n"
     ]
    }
   ],
   "source": [
    "target_col = target_outcome # choose the appropriate target variable\n",
    "\n",
    "train_set, test_set, scaling_params = prepare_data(compustat_pre_cleaned, feature_cols, target_col, log_cols, standardize_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "813ce55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan_rows_test(df, cols, how='any'):\n",
    "    \"\"\"\n",
    "    Remove rows containing NaN in specified columns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The dataset\n",
    "    cols : list\n",
    "        List of column names to check for NaN\n",
    "    how : str, default 'any'\n",
    "        - 'any': Remove row if ANY of the specified columns has NaN\n",
    "        - 'all': Remove row only if ALL of the specified columns have NaN\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame with rows removed\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    # Remove rows where any of these columns is NaN\n",
    "    df_clean = remove_nan_rows(df, cols=['revt', 'at', 'ni'], how='any')\n",
    "    \n",
    "    # Remove rows only where ALL of these columns are NaN\n",
    "    df_clean = remove_nan_rows(df, cols=['revt', 'at', 'ni'], how='all')\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter to columns that exist\n",
    "    cols_exist = [c for c in cols if c in df.columns]\n",
    "    cols_missing = [c for c in cols if c not in df.columns]\n",
    "    \n",
    "    if cols_missing:\n",
    "        print(f\"Warning: Columns not found (ignored): {cols_missing}\")\n",
    "    \n",
    "    if not cols_exist:\n",
    "        print(\"No valid columns specified. Returning original DataFrame.\")\n",
    "        return df\n",
    "    \n",
    "    # Count NaN before removal\n",
    "    rows_before = len(df)\n",
    "    \n",
    "    # Per-column NaN counts\n",
    "    print(\"NaN counts per column before removal:\")\n",
    "    print(\"-\" * 40)\n",
    "    for col in cols_exist:\n",
    "        nan_count = df[col].isna().sum()\n",
    "        pct = nan_count / rows_before * 100\n",
    "        print(f\"  {col}: {nan_count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "e6e80909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN counts per column before removal:\n",
      "----------------------------------------\n",
      "  revt: 0 (0.0%)\n",
      "  oibdp: 0 (0.0%)\n",
      "  ni: 0 (0.0%)\n",
      "  xint: 0 (0.0%)\n",
      "  dp: 0 (0.0%)\n",
      "  at: 0 (0.0%)\n",
      "  act: 0 (0.0%)\n",
      "  che: 0 (0.0%)\n",
      "  lt: 0 (0.0%)\n",
      "  lct: 0 (0.0%)\n",
      "  dlc: 0 (0.0%)\n",
      "  dltt: 0 (0.0%)\n",
      "  ceq: 0 (0.0%)\n",
      "  dv: 0 (0.0%)\n",
      "  cogs: 0 (0.0%)\n",
      "  ebit: 0 (0.0%)\n",
      "  re: 0 (0.0%)\n",
      "  rect: 0 (0.0%)\n",
      "  invt: 0 (0.0%)\n",
      "  oancf: 0 (0.0%)\n",
      "  capx: 0 (0.0%)\n",
      "  fincf: 0 (0.0%)\n",
      "  ivncf: 0 (0.0%)\n",
      "  prcc_f: 0 (0.0%)\n",
      "  ap: 0 (0.0%)\n",
      "  gross_margin: 0 (0.0%)\n",
      "  net_profit_margin: 0 (0.0%)\n",
      "  roa: 0 (0.0%)\n",
      "  roe: 0 (0.0%)\n",
      "  asset_turnover: 0 (0.0%)\n",
      "  cash_to_assets: 0 (0.0%)\n",
      "  fixed_asset_intensity: 0 (0.0%)\n",
      "  current_ratio: 0 (0.0%)\n",
      "  quick_ratio: 0 (0.0%)\n",
      "  total_debt: 0 (0.0%)\n",
      "  debt_to_assets: 0 (0.0%)\n",
      "  debt_to_equity: 0 (0.0%)\n",
      "  liabilities_to_assets: 0 (0.0%)\n",
      "  interest_coverage: 0 (0.0%)\n",
      "  long_term_debt_ratio: 0 (0.0%)\n",
      "  dividend_payout_ratio: 0 (0.0%)\n",
      "  dividend_yield: 0 (0.0%)\n",
      "  retention_ratio: 0 (0.0%)\n",
      "  revt_1_year_pct_change: 0 (0.0%)\n",
      "  at_1_year_pct_change: 0 (0.0%)\n",
      "  ni_1_year_pct_change: 0 (0.0%)\n",
      "  oibdp_1_year_pct_change: 0 (0.0%)\n",
      "  ceq_1_year_pct_change: 0 (0.0%)\n",
      "  total_debt_1_year_pct_change: 0 (0.0%)\n",
      "  revt_2_year_pct_change: 0 (0.0%)\n",
      "  at_2_year_pct_change: 0 (0.0%)\n",
      "  ni_2_year_pct_change: 0 (0.0%)\n",
      "  oibdp_2_year_pct_change: 0 (0.0%)\n",
      "  ceq_2_year_pct_change: 0 (0.0%)\n",
      "  total_debt_2_year_pct_change: 0 (0.0%)\n",
      "  revt_5_year_pct_change: 0 (0.0%)\n",
      "  at_5_year_pct_change: 0 (0.0%)\n",
      "  ni_5_year_pct_change: 0 (0.0%)\n",
      "  oibdp_5_year_pct_change: 0 (0.0%)\n",
      "  ceq_5_year_pct_change: 0 (0.0%)\n",
      "  total_debt_5_year_pct_change: 0 (0.0%)\n",
      "  capex_to_assets: 0 (0.0%)\n",
      "  investing_cash_flow_to_assets: 0 (0.0%)\n",
      "  days_sales_outstanding: 0 (0.0%)\n",
      "  days_inventory_outstanding: 0 (0.0%)\n",
      "  net_working_capital_to_assets: 0 (0.0%)\n",
      "  free_cash_flow_to_sales: 0 (0.0%)\n",
      "  financing_cash_flow_to_assets: 0 (0.0%)\n",
      "  market_capitalization: 0 (0.0%)\n",
      "  days_payables_outstanding: 0 (0.0%)\n",
      "  cash_conversion_cycle: 0 (0.0%)\n",
      "  capx_1_year_pct_change: 0 (0.0%)\n",
      "  market_capitalization_1_year_pct_change: 0 (0.0%)\n",
      "  free_cash_flow_to_sales_1_year_pct_change: 0 (0.0%)\n",
      "  financing_cash_flow_to_assets_1_year_pct_change: 0 (0.0%)\n",
      "  capx_2_year_pct_change: 0 (0.0%)\n",
      "  market_capitalization_2_year_pct_change: 0 (0.0%)\n",
      "  free_cash_flow_to_sales_2_year_pct_change: 0 (0.0%)\n",
      "  financing_cash_flow_to_assets_2_year_pct_change: 0 (0.0%)\n",
      "  capx_5_year_pct_change: 0 (0.0%)\n",
      "  market_capitalization_5_year_pct_change: 0 (0.0%)\n",
      "  free_cash_flow_to_sales_5_year_pct_change: 0 (0.0%)\n",
      "  financing_cash_flow_to_assets_5_year_pct_change: 0 (0.0%)\n",
      "  cash_conversion_cycle_1_year_pct_change: 0 (0.0%)\n",
      "  cash_conversion_cycle_2_year_pct_change: 0 (0.0%)\n",
      "  cash_conversion_cycle_5_year_pct_change: 0 (0.0%)\n",
      "  book_value_per_share: 0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "remove_nan_rows_test(train_set, feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7cba45",
   "metadata": {},
   "source": [
    "# Saving test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "e49aa07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if columns_set == 'less':\n",
    "    feat = \"less_features_\"\n",
    "else:\n",
    "    feat = ''\n",
    "\n",
    "if crisis_period == 'y':\n",
    "    crisis_ = 'crisis_period_'\n",
    "else:\n",
    "    crisis_ = ''\n",
    "\n",
    "if target_industry:\n",
    "    crisis_ += f\"{target_industry}_\"  \n",
    "\n",
    "train_set.to_csv(\n",
    "    f\"{crisis_}{feat}train_set_{target_outcome}.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "test_set.to_csv(\n",
    "    f\"{crisis_}{feat}test_set_{target_outcome}.csv\",\n",
    "    index=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
