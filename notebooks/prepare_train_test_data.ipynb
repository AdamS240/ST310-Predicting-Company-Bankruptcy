{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea0e0067",
   "metadata": {},
   "source": [
    "# Prepare the train and the test data (set parameters in cells 17 & 23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bd9c41",
   "metadata": {},
   "source": [
    "This code does the following:\n",
    "1. adds the nacis groupings \n",
    "2. adds default flags (default next year, default in 2 years, defaulted in the past, will ever default in the future)\n",
    "3. removes observations for which there was a default within the last 3 years\n",
    "4. removes NaN according to column selection\n",
    "5. logs and standardizes data\n",
    "\n",
    "Important parameters in section below:\n",
    "chosing which columns matter for NaN: \"cols_for_NaN_removal\" (cell 17)\n",
    "chosing target column (what will be your y value): \"target_col\" (cell 23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1ed0d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e62b5413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 493572 entries, 0 to 493571\n",
      "Data columns (total 91 columns):\n",
      " #   Column                                           Non-Null Count   Dtype  \n",
      "---  ------                                           --------------   -----  \n",
      " 0   gvkey                                            493572 non-null  int64  \n",
      " 1   datadate                                         493572 non-null  object \n",
      " 2   fyear                                            493572 non-null  int64  \n",
      " 3   conm                                             493572 non-null  object \n",
      " 4   tic                                              493367 non-null  object \n",
      " 5   naicsh                                           359039 non-null  float64\n",
      " 6   revt                                             397144 non-null  float64\n",
      " 7   cogs                                             356254 non-null  float64\n",
      " 8   ebit                                             353650 non-null  float64\n",
      " 9   oibdp                                            351536 non-null  float64\n",
      " 10  re                                               388321 non-null  float64\n",
      " 11  ni                                               356339 non-null  float64\n",
      " 12  xint                                             377710 non-null  float64\n",
      " 13  dp                                               385535 non-null  float64\n",
      " 14  at                                               399718 non-null  float64\n",
      " 15  act                                              334732 non-null  float64\n",
      " 16  che                                              358092 non-null  float64\n",
      " 17  rect                                             350810 non-null  float64\n",
      " 18  invt                                             362994 non-null  float64\n",
      " 19  lt                                               398899 non-null  float64\n",
      " 20  lct                                              338955 non-null  float64\n",
      " 21  dlc                                              396460 non-null  float64\n",
      " 22  dltt                                             398354 non-null  float64\n",
      " 23  ceq                                              398870 non-null  float64\n",
      " 24  csho                                             420594 non-null  float64\n",
      " 25  oancf                                            294929 non-null  float64\n",
      " 26  capx                                             351059 non-null  float64\n",
      " 27  fincf                                            294993 non-null  float64\n",
      " 28  ivncf                                            294992 non-null  float64\n",
      " 29  dv                                               349228 non-null  float64\n",
      " 30  prcc_f                                           420604 non-null  float64\n",
      " 31  at_fn                                            95871 non-null   object \n",
      " 32  TL_flag                                          493572 non-null  int64  \n",
      " 33  gross_margin                                     323155 non-null  float64\n",
      " 34  net_profit_margin                                327725 non-null  float64\n",
      " 35  roa                                              355011 non-null  float64\n",
      " 36  roe                                              355222 non-null  float64\n",
      " 37  asset_turnover                                   395787 non-null  float64\n",
      " 38  cash_to_assets                                   356875 non-null  float64\n",
      " 39  fixed_asset_intensity                            333521 non-null  float64\n",
      " 40  net_working_capital_to_assets                    333175 non-null  float64\n",
      " 41  capex_to_assets                                  349859 non-null  float64\n",
      " 42  current_ratio                                    333855 non-null  float64\n",
      " 43  quick_ratio                                      336215 non-null  float64\n",
      " 44  total_debt                                       395698 non-null  float64\n",
      " 45  debt_to_assets                                   394483 non-null  float64\n",
      " 46  debt_to_equity                                   394619 non-null  float64\n",
      " 47  liabilities_to_assets                            397683 non-null  float64\n",
      " 48  interest_coverage                                295980 non-null  float64\n",
      " 49  long_term_debt_ratio                             397273 non-null  float64\n",
      " 50  book_value_per_share                             349684 non-null  float64\n",
      " 51  earnings_per_share                               348300 non-null  float64\n",
      " 52  dividend_payout_ratio                            348891 non-null  float64\n",
      " 53  dividend_yield                                   348415 non-null  float64\n",
      " 54  retention_ratio                                  348891 non-null  float64\n",
      " 55  market_capitalization                            373045 non-null  float64\n",
      " 56  free_cash_flow_to_sales                          265408 non-null  float64\n",
      " 57  financing_cash_flow_to_assets                    293829 non-null  float64\n",
      " 58  investing_cash_flow_to_assets                    293828 non-null  float64\n",
      " 59  days_sales_outstanding                           321599 non-null  float64\n",
      " 60  days_inventory_outstanding                       326992 non-null  float64\n",
      " 61  revt_1_year_pct_change                           336026 non-null  float64\n",
      " 62  at_1_year_pct_change                             362852 non-null  float64\n",
      " 63  ni_1_year_pct_change                             322377 non-null  float64\n",
      " 64  oibdp_1_year_pct_change                          317324 non-null  float64\n",
      " 65  ceq_1_year_pct_change                            362717 non-null  float64\n",
      " 66  total_debt_1_year_pct_change                     305861 non-null  float64\n",
      " 67  capx_1_year_pct_change                           291956 non-null  float64\n",
      " 68  market_capitalization_1_year_pct_change          332630 non-null  float64\n",
      " 69  free_cash_flow_to_sales_1_year_pct_change        236377 non-null  float64\n",
      " 70  financing_cash_flow_to_assets_1_year_pct_change  257512 non-null  float64\n",
      " 71  revt_2_year_pct_change                           305417 non-null  float64\n",
      " 72  at_2_year_pct_change                             328965 non-null  float64\n",
      " 73  ni_2_year_pct_change                             291316 non-null  float64\n",
      " 74  oibdp_2_year_pct_change                          286600 non-null  float64\n",
      " 75  ceq_2_year_pct_change                            328694 non-null  float64\n",
      " 76  total_debt_2_year_pct_change                     276199 non-null  float64\n",
      " 77  capx_2_year_pct_change                           264658 non-null  float64\n",
      " 78  market_capitalization_2_year_pct_change          297153 non-null  float64\n",
      " 79  free_cash_flow_to_sales_2_year_pct_change        211544 non-null  float64\n",
      " 80  financing_cash_flow_to_assets_2_year_pct_change  231390 non-null  float64\n",
      " 81  revt_5_year_pct_change                           229791 non-null  float64\n",
      " 82  at_5_year_pct_change                             245587 non-null  float64\n",
      " 83  ni_5_year_pct_change                             215527 non-null  float64\n",
      " 84  oibdp_5_year_pct_change                          212147 non-null  float64\n",
      " 85  ceq_5_year_pct_change                            245171 non-null  float64\n",
      " 86  total_debt_5_year_pct_change                     204433 non-null  float64\n",
      " 87  capx_5_year_pct_change                           197596 non-null  float64\n",
      " 88  market_capitalization_5_year_pct_change          215224 non-null  float64\n",
      " 89  free_cash_flow_to_sales_5_year_pct_change        153652 non-null  float64\n",
      " 90  financing_cash_flow_to_assets_5_year_pct_change  168156 non-null  float64\n",
      "dtypes: float64(84), int64(3), object(4)\n",
      "memory usage: 342.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# set the path to the file obtained from cleandata.ipynb\n",
    "compustat_pre_cleaned_path = \"/Users/songyi/Documents/st310/st310_groupproject/ST310-Predicting-Company-Bankruptcy/notebooks/compustat_df_1980_deduplicated_extended_winsor.csv\"\n",
    "\n",
    "compustat_pre_cleaned = pd.read_csv(compustat_pre_cleaned_path)\n",
    "compustat_pre_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcff7ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gvkey',\n",
       " 'datadate',\n",
       " 'fyear',\n",
       " 'conm',\n",
       " 'tic',\n",
       " 'naicsh',\n",
       " 'revt',\n",
       " 'cogs',\n",
       " 'ebit',\n",
       " 'oibdp',\n",
       " 're',\n",
       " 'ni',\n",
       " 'xint',\n",
       " 'dp',\n",
       " 'at',\n",
       " 'act',\n",
       " 'che',\n",
       " 'rect',\n",
       " 'invt',\n",
       " 'lt',\n",
       " 'lct',\n",
       " 'dlc',\n",
       " 'dltt',\n",
       " 'ceq',\n",
       " 'csho',\n",
       " 'oancf',\n",
       " 'capx',\n",
       " 'fincf',\n",
       " 'ivncf',\n",
       " 'dv',\n",
       " 'prcc_f',\n",
       " 'at_fn',\n",
       " 'TL_flag',\n",
       " 'gross_margin',\n",
       " 'net_profit_margin',\n",
       " 'roa',\n",
       " 'roe',\n",
       " 'asset_turnover',\n",
       " 'cash_to_assets',\n",
       " 'fixed_asset_intensity',\n",
       " 'net_working_capital_to_assets',\n",
       " 'capex_to_assets',\n",
       " 'current_ratio',\n",
       " 'quick_ratio',\n",
       " 'total_debt',\n",
       " 'debt_to_assets',\n",
       " 'debt_to_equity',\n",
       " 'liabilities_to_assets',\n",
       " 'interest_coverage',\n",
       " 'long_term_debt_ratio',\n",
       " 'book_value_per_share',\n",
       " 'earnings_per_share',\n",
       " 'dividend_payout_ratio',\n",
       " 'dividend_yield',\n",
       " 'retention_ratio',\n",
       " 'market_capitalization',\n",
       " 'free_cash_flow_to_sales',\n",
       " 'financing_cash_flow_to_assets',\n",
       " 'investing_cash_flow_to_assets',\n",
       " 'days_sales_outstanding',\n",
       " 'days_inventory_outstanding',\n",
       " 'revt_1_year_pct_change',\n",
       " 'at_1_year_pct_change',\n",
       " 'ni_1_year_pct_change',\n",
       " 'oibdp_1_year_pct_change',\n",
       " 'ceq_1_year_pct_change',\n",
       " 'total_debt_1_year_pct_change',\n",
       " 'capx_1_year_pct_change',\n",
       " 'market_capitalization_1_year_pct_change',\n",
       " 'free_cash_flow_to_sales_1_year_pct_change',\n",
       " 'financing_cash_flow_to_assets_1_year_pct_change',\n",
       " 'revt_2_year_pct_change',\n",
       " 'at_2_year_pct_change',\n",
       " 'ni_2_year_pct_change',\n",
       " 'oibdp_2_year_pct_change',\n",
       " 'ceq_2_year_pct_change',\n",
       " 'total_debt_2_year_pct_change',\n",
       " 'capx_2_year_pct_change',\n",
       " 'market_capitalization_2_year_pct_change',\n",
       " 'free_cash_flow_to_sales_2_year_pct_change',\n",
       " 'financing_cash_flow_to_assets_2_year_pct_change',\n",
       " 'revt_5_year_pct_change',\n",
       " 'at_5_year_pct_change',\n",
       " 'ni_5_year_pct_change',\n",
       " 'oibdp_5_year_pct_change',\n",
       " 'ceq_5_year_pct_change',\n",
       " 'total_debt_5_year_pct_change',\n",
       " 'capx_5_year_pct_change',\n",
       " 'market_capitalization_5_year_pct_change',\n",
       " 'free_cash_flow_to_sales_5_year_pct_change',\n",
       " 'financing_cash_flow_to_assets_5_year_pct_change']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compustat_pre_cleaned.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5dc453",
   "metadata": {},
   "source": [
    "# Naics groupings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af2fddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_naics_industry_group(df, naics_col='naicsh'):\n",
    "    \"\"\"\n",
    "    Add industry group classification based on 2-digit NAICS codes.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        Must contain the naics_col column\n",
    "    naics_col : str, default 'naicsh'\n",
    "        Column name containing NAICS codes\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame with added columns:\n",
    "        - naics_2_digit: First 2 digits of NAICS code\n",
    "        - naics_industry_group: Industry group classification string\n",
    "    \n",
    "    Industry Groups:\n",
    "    ----------------\n",
    "    - Capital Intensive: 11, 21, 22, 23, 31-33, 48-49\n",
    "    - Trade: 42, 44-45\n",
    "    - Financial and Real Estate: 52, 53\n",
    "    - Administrative: 54, 55, 56, 81, 92\n",
    "    - Healthcare and Education: 61, 62\n",
    "    - Food Services and Entertainment: 71, 72\n",
    "    - Other: All other codes\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Extract first 2 digits of NAICS code\n",
    "    # Handle float (e.g., 523100.0) by converting to int first, then string\n",
    "    df['naics_2_digit'] = (\n",
    "        df[naics_col]\n",
    "        .apply(lambda x: str(int(x))[:2] if pd.notna(x) else np.nan)\n",
    "    )\n",
    "    \n",
    "    # Convert to numeric for easier comparison\n",
    "    df['_naics_2_numeric'] = pd.to_numeric(df['naics_2_digit'], errors='coerce')\n",
    "    \n",
    "    # Define industry group mapping\n",
    "    def classify_industry(code):\n",
    "        if pd.isna(code):\n",
    "            return np.nan\n",
    "        \n",
    "        code = int(code)\n",
    "        \n",
    "        # Capital Intensive: 11, 21, 22, 23, 31-33, 48-49\n",
    "        if code in [11, 21, 22, 23] or (31 <= code <= 33) or (48 <= code <= 49):\n",
    "            return 'Capital Intensive'\n",
    "        \n",
    "        # Trade: 42, 44-45\n",
    "        elif code == 42 or (44 <= code <= 45):\n",
    "            return 'Trade'\n",
    "        \n",
    "        # Financial and Real Estate: 52, 53\n",
    "        elif code in [52, 53]:\n",
    "            return 'Financial and Real Estate'\n",
    "        \n",
    "        # Administrative: 54, 55, 56, 81, 92\n",
    "        elif code in [54, 55, 56, 81, 92]:\n",
    "            return 'Administrative'\n",
    "        \n",
    "        # Healthcare and Education: 61, 62\n",
    "        elif code in [61, 62]:\n",
    "            return 'Healthcare and Education'\n",
    "        \n",
    "        # Food Services and Entertainment: 71, 72\n",
    "        elif code in [71, 72]:\n",
    "            return 'Food Services and Entertainment'\n",
    "        \n",
    "        else:\n",
    "            return 'Other'\n",
    "    \n",
    "    df['naics_industry_group'] = df['_naics_2_numeric'].apply(classify_industry)\n",
    "    \n",
    "    # Drop helper column\n",
    "    df = df.drop(columns=['_naics_2_numeric'])\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"NAICS Industry Group Summary:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    total = len(df)\n",
    "    non_null = df['naics_industry_group'].notna().sum()\n",
    "    \n",
    "    print(f\"Total observations: {total:,}\")\n",
    "    print(f\"Observations with NAICS code: {non_null:,}\")\n",
    "    print(f\"Missing NAICS: {total - non_null:,}\")\n",
    "    print()\n",
    "    \n",
    "    group_counts = df['naics_industry_group'].value_counts(dropna=False)\n",
    "    print(\"Distribution by industry group:\")\n",
    "    for group, count in group_counts.items():\n",
    "        pct = count / total * 100\n",
    "        group_name = group if pd.notna(group) else 'Missing'\n",
    "        print(f\"  {group_name}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "387a1979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAICS Industry Group Summary:\n",
      "------------------------------------------------------------\n",
      "Total observations: 493,572\n",
      "Observations with NAICS code: 359,039\n",
      "Missing NAICS: 134,533\n",
      "\n",
      "Distribution by industry group:\n",
      "  Capital Intensive: 182,804 (37.0%)\n",
      "  Missing: 134,533 (27.3%)\n",
      "  Financial and Real Estate: 72,602 (14.7%)\n",
      "  Other: 39,710 (8.0%)\n",
      "  Trade: 25,172 (5.1%)\n",
      "  Administrative: 22,507 (4.6%)\n",
      "  Food Services and Entertainment: 8,937 (1.8%)\n",
      "  Healthcare and Education: 7,307 (1.5%)\n"
     ]
    }
   ],
   "source": [
    "compustat_pre_cleaned = add_naics_industry_group(compustat_pre_cleaned, naics_col='naicsh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3de7617",
   "metadata": {},
   "source": [
    "# Default flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c5ce439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_default_flags(df, default_indicator='TL_flag', default_value=1):\n",
    "    \"\"\"\n",
    "    Add multiple default-related columns to the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas DataFrame\n",
    "        Must contain columns: gvkey, fyear, and the default_indicator column\n",
    "    default_indicator : str, default 'TL_flag'\n",
    "        Column name that indicates default status (1 = default, 0 = no default)\n",
    "    default_value : int or float, default 1\n",
    "        Value in default_indicator that represents a default event\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas DataFrame with added columns:\n",
    "        - default_next_1y\n",
    "        - default_next_2y\n",
    "        - default_past_3y_plus\n",
    "        - default_ever_future\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(['gvkey', 'fyear']).reset_index(drop=True)\n",
    "\n",
    "    # ============================================================\n",
    "    # Helper: all default events\n",
    "    # ============================================================\n",
    "    defaults = (\n",
    "        df.loc[df[default_indicator] == default_value, ['gvkey', 'fyear']]\n",
    "        .rename(columns={'fyear': 'default_year'})\n",
    "    )\n",
    "\n",
    "    # ============================================================\n",
    "    # 1) DEFAULT IN NEXT 1 YEAR (fyear + 1)\n",
    "    # ============================================================\n",
    "    next_1y = (\n",
    "        df[['gvkey', 'fyear', default_indicator]]\n",
    "        .assign(fyear=lambda x: x['fyear'] - 1)\n",
    "        .rename(columns={default_indicator: 'default_next_1y'})\n",
    "    )\n",
    "\n",
    "    df = df.merge(next_1y, on=['gvkey', 'fyear'], how='left')\n",
    "\n",
    "    df['default_next_1y'] = np.where(\n",
    "        df['default_next_1y'].isna(),\n",
    "        np.nan,\n",
    "        (df['default_next_1y'] == default_value).astype(float)\n",
    "    )\n",
    "\n",
    "    # ============================================================\n",
    "    # 2) DEFAULT IN NEXT 2 YEARS (fyear + 1 OR fyear + 2)\n",
    "    # ============================================================\n",
    "    y1 = (\n",
    "        df[['gvkey', 'fyear', default_indicator]]\n",
    "        .assign(fyear=lambda x: x['fyear'] - 1)\n",
    "        .rename(columns={default_indicator: 'y1'})\n",
    "    )\n",
    "\n",
    "    y2 = (\n",
    "        df[['gvkey', 'fyear', default_indicator]]\n",
    "        .assign(fyear=lambda x: x['fyear'] - 2)\n",
    "        .rename(columns={default_indicator: 'y2'})\n",
    "    )\n",
    "\n",
    "    df = df.merge(y1, on=['gvkey', 'fyear'], how='left')\n",
    "    df = df.merge(y2, on=['gvkey', 'fyear'], how='left')\n",
    "\n",
    "    has_any_year = df[['y1', 'y2']].notna().any(axis=1)\n",
    "    has_default = (\n",
    "        (df['y1'] == default_value) |\n",
    "        (df['y2'] == default_value)\n",
    "    )\n",
    "\n",
    "    df['default_next_2y'] = np.where(\n",
    "        ~has_any_year,\n",
    "        np.nan,\n",
    "        has_default.astype(float)\n",
    "    )\n",
    "\n",
    "    df = df.drop(columns=['y1', 'y2'])\n",
    "\n",
    "    # ============================================================\n",
    "    # 3) DEFAULT MORE THAN 3 YEARS AGO (fyear - 4 or earlier)\n",
    "    # ============================================================\n",
    "    past = df.merge(defaults, on='gvkey', how='left')\n",
    "\n",
    "    past['past_default'] = (past['fyear'] - past['default_year']) > 3\n",
    "\n",
    "    past_flag = (\n",
    "        past.groupby(['gvkey', 'fyear'])['past_default']\n",
    "        .any()\n",
    "        .astype(float)\n",
    "        .reset_index(name='default_past_3y_plus')\n",
    "    )\n",
    "\n",
    "    df = df.merge(past_flag, on=['gvkey', 'fyear'], how='left')\n",
    "    df['default_past_3y_plus'] = df['default_past_3y_plus'].fillna(0.0)\n",
    "\n",
    "    # ============================================================\n",
    "    # 4) DEFAULT EVER IN FUTURE (any year > fyear)\n",
    "    # ============================================================\n",
    "    future = df[['gvkey', 'fyear']].merge(defaults, on='gvkey', how='left')\n",
    "\n",
    "    future['future_default'] = (future['default_year'] > future['fyear'])\n",
    "\n",
    "    future_flag = (\n",
    "        future.groupby(['gvkey', 'fyear'])['future_default']\n",
    "        .any()\n",
    "        .reset_index(name='default_ever_future')\n",
    "    )\n",
    "\n",
    "    df = df.merge(future_flag, on=['gvkey', 'fyear'], how='left')\n",
    "\n",
    "    # Identify whether future years exist (not just defaults)\n",
    "    max_fyear = df.groupby('gvkey')['fyear'].transform('max')\n",
    "\n",
    "    df['default_ever_future'] = np.where(\n",
    "        df['fyear'] == max_fyear,\n",
    "        np.nan,                      # no future years exist\n",
    "        df['default_ever_future'].astype(float)\n",
    "    )\n",
    "\n",
    "    # ============================================================\n",
    "    # SUMMARY (optional diagnostic output)\n",
    "    # ============================================================\n",
    "    print(\"Default Flags Summary:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for col in [\n",
    "        'default_next_1y',\n",
    "        'default_next_2y',\n",
    "        'default_past_3y_plus',\n",
    "        'default_ever_future'\n",
    "    ]:\n",
    "        non_null = df[col].notna().sum()\n",
    "        defaults_n = (df[col] == 1).sum()\n",
    "        rate = defaults_n / non_null * 100 if non_null > 0 else 0\n",
    "\n",
    "        print(f\"{col}:\")\n",
    "        print(f\"  Non-null observations: {non_null:,}\")\n",
    "        print(f\"  Defaults (=1): {defaults_n:,} ({rate:.2f}%)\\n\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "457543d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Flags Summary:\n",
      "------------------------------------------------------------\n",
      "default_next_1y:\n",
      "  Non-null observations: 448,882\n",
      "  Defaults (=1): 1,735 (0.39%)\n",
      "\n",
      "default_next_2y:\n",
      "  Non-null observations: 448,967\n",
      "  Defaults (=1): 2,623 (0.58%)\n",
      "\n",
      "default_past_3y_plus:\n",
      "  Non-null observations: 493,572\n",
      "  Defaults (=1): 3,606 (0.73%)\n",
      "\n",
      "default_ever_future:\n",
      "  Non-null observations: 449,054\n",
      "  Defaults (=1): 9,374 (2.09%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compustat_pre_cleaned = add_default_flags(compustat_pre_cleaned, default_indicator='TL_flag', default_value=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66b627c",
   "metadata": {},
   "source": [
    "# Removing periods between defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcf06d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_post_default_observations(df, default_indicator='TL_flag', default_value=1, lookback_years=3):\n",
    "    \"\"\"\n",
    "    Remove observations if the firm had a default in the previous N years.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        Must contain columns: gvkey, fyear, and the default_indicator column\n",
    "    default_indicator : str, default 'TL_flag'\n",
    "        Column name that indicates default status\n",
    "    default_value : int/float, default 1\n",
    "        Value in default_indicator that represents a default event\n",
    "    lookback_years : int, default 3\n",
    "        Number of years to look back for prior defaults\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame with post-default observations removed\n",
    "    \n",
    "    Notes:\n",
    "    ------\n",
    "    - For each observation, checks if the firm had a default in any of the\n",
    "      previous `lookback_years` years (fyear-1 through fyear-lookback_years)\n",
    "    - Removes the observation if any prior default is found\n",
    "    - The default year itself is NOT removed (only subsequent years)\n",
    "    - Handles gaps in data correctly (only looks at years that exist)\n",
    "    \n",
    "    Example (lookback_years=5):\n",
    "    --------\n",
    "    If firm A has:\n",
    "        fyear=2018, TL_flag=0  -> KEEP (no prior default)\n",
    "        fyear=2019, TL_flag=1  -> KEEP (this is the default year)\n",
    "        fyear=2020, TL_flag=0  -> REMOVE (default in 2019, within 5 years)\n",
    "        fyear=2021, TL_flag=0  -> REMOVE (default in 2019, within 5 years)\n",
    "        fyear=2022, TL_flag=0  -> REMOVE (default in 2019, within 5 years)\n",
    "        fyear=2023, TL_flag=0  -> REMOVE (default in 2019, within 5 years)\n",
    "        fyear=2024, TL_flag=0  -> REMOVE (default in 2019, within 5 years)\n",
    "        fyear=2025, TL_flag=0  -> KEEP (2019 is now 6 years ago)\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Sort by company and fiscal year\n",
    "    df = df.sort_values(['gvkey', 'fyear']).reset_index(drop=True)\n",
    "    \n",
    "    # Get all default events\n",
    "    defaults = df[df[default_indicator] == default_value][['gvkey', 'fyear']].copy()\n",
    "    defaults = defaults.rename(columns={'fyear': 'default_year'})\n",
    "    \n",
    "    # For each observation, check if there's a default in the lookback window\n",
    "    # Merge all defaults for each company\n",
    "    df_with_defaults = df.merge(defaults, on='gvkey', how='left')\n",
    "    \n",
    "    # Calculate years since default\n",
    "    df_with_defaults['years_since_default'] = df_with_defaults['fyear'] - df_with_defaults['default_year']\n",
    "    \n",
    "    # Flag observations where a default occurred in the lookback window\n",
    "    # (years_since_default between 1 and lookback_years, inclusive)\n",
    "    df_with_defaults['has_recent_default'] = (\n",
    "        (df_with_defaults['years_since_default'] >= 1) & \n",
    "        (df_with_defaults['years_since_default'] <= lookback_years)\n",
    "    )\n",
    "    \n",
    "    # Group by original observation and check if ANY prior default exists\n",
    "    recent_default_flag = df_with_defaults.groupby(['gvkey', 'fyear'])['has_recent_default'].any().reset_index()\n",
    "    recent_default_flag = recent_default_flag.rename(columns={'has_recent_default': 'remove_flag'})\n",
    "    \n",
    "    # Merge back to original dataframe\n",
    "    df = df.merge(recent_default_flag, on=['gvkey', 'fyear'], how='left')\n",
    "    \n",
    "    # Fill NaN (firms with no defaults at all) with False\n",
    "    df['remove_flag'] = df['remove_flag'].fillna(False)\n",
    "    \n",
    "    # Print summary before removal\n",
    "    total_obs = len(df)\n",
    "    obs_to_remove = df['remove_flag'].sum()\n",
    "    firms_affected = df[df['remove_flag']]['gvkey'].nunique()\n",
    "    \n",
    "    print(\"Post-Default Observation Removal Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Lookback window: {lookback_years} years\")\n",
    "    print(f\"Total observations before: {total_obs:,}\")\n",
    "    print(f\"Observations to remove: {obs_to_remove:,}\")\n",
    "    print(f\"Firms affected: {firms_affected:,}\")\n",
    "    print(f\"Observations remaining: {total_obs - obs_to_remove:,}\")\n",
    "    print(f\"Percentage removed: {obs_to_remove / total_obs * 100:.2f}%\")\n",
    "    \n",
    "    # Remove flagged observations\n",
    "    df_clean = df[~df['remove_flag']].drop(columns=['remove_flag']).reset_index(drop=True)\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c25a6d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-Default Observation Removal Summary:\n",
      "--------------------------------------------------\n",
      "Lookback window: 3 years\n",
      "Total observations before: 493,572\n",
      "Observations to remove: 2,353\n",
      "Firms affected: 729\n",
      "Observations remaining: 491,219\n",
      "Percentage removed: 0.48%\n"
     ]
    }
   ],
   "source": [
    "compustat_pre_cleaned = remove_post_default_observations(compustat_pre_cleaned, default_indicator='TL_flag', default_value=1, lookback_years=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06b00cc",
   "metadata": {},
   "source": [
    "# Remove NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a89730",
   "metadata": {},
   "source": [
    "Important: in cell 17, define which columns matter for you when removing NaN values. For example, if you do not care about missing values in columns like \"revt_5_year_pct_change\", do not include it in the list. \n",
    "\n",
    "The function remove_nan_rows takes in a list of columns as an argument, and removes all rows, which have a NaN entry in at least one of the specified columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6566271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all potential features\n",
    "all_feature_cols = [\n",
    "    # default flag\n",
    "    'default_next_1y',\n",
    "    # Liquidity\n",
    "    'current_ratio',\n",
    "    'quick_ratio',\n",
    "    'net_working_capital_to_assets',\n",
    "\n",
    "    # Leverage\n",
    "    'debt_to_equity',\n",
    "    'liabilities_to_assets',\n",
    "    'interest_coverage',\n",
    "    'total_debt',\n",
    "    'debt_to_assets',\n",
    "\n",
    "    # Market value\n",
    "    'book_value_per_share',\n",
    "    'earnings_per_share',\n",
    "    'dividend_payout_ratio',\n",
    "    'dividend_yield',\n",
    "    'retention_ratio',\n",
    "\n",
    "    # Profitability\n",
    "    'gross_margin',\n",
    "    'net_profit_margin',\n",
    "    'roa',\n",
    "    'roe',\n",
    "\n",
    "    # Efficiency\n",
    "    'asset_turnover',\n",
    "    'capex_to_assets',\n",
    "    'days_sales_outstanding',\n",
    "    'days_inventory_outstanding',\n",
    "\n",
    "    # Cash flow (if you want them enforced)\n",
    "    'free_cash_flow_to_sales',\n",
    "    'financing_cash_flow_to_assets',\n",
    "    'investing_cash_flow_to_assets'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3738c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def remove_nan_rows(df, cols, how=\\'any\\'):\\n    \"\"\"\\n    Remove rows containing NaN in specified columns.\\n    \\n    Parameters:\\n    -----------\\n    df : pandas DataFrame\\n        The dataset\\n    cols : list\\n        List of column names to check for NaN\\n    how : str, default \\'any\\'\\n        - \\'any\\': Remove row if ANY of the specified columns has NaN\\n        - \\'all\\': Remove row only if ALL of the specified columns have NaN\\n    \\n    Returns:\\n    --------\\n    pandas DataFrame with rows removed\\n    \\n    Example:\\n    --------\\n    # Remove rows where any of these columns is NaN\\n    df_clean = remove_nan_rows(df, cols=[\\'revt\\', \\'at\\', \\'ni\\'], how=\\'any\\')\\n    \\n    # Remove rows only where ALL of these columns are NaN\\n    df_clean = remove_nan_rows(df, cols=[\\'revt\\', \\'at\\', \\'ni\\'], how=\\'all\\')\\n    \"\"\"\\n    \\n    # Filter to columns that exist\\n    cols_exist = [c for c in cols if c in df.columns]\\n    cols_missing = [c for c in cols if c not in df.columns]\\n    \\n    if cols_missing:\\n        print(f\"Warning: Columns not found (ignored): {cols_missing}\")\\n    \\n    if not cols_exist:\\n        print(\"No valid columns specified. Returning original DataFrame.\")\\n        return df\\n    \\n    # Count NaN before removal\\n    rows_before = len(df)\\n    \\n    # Per-column NaN counts\\n    print(\"NaN counts per column before removal:\")\\n    print(\"-\" * 40)\\n    for col in cols_exist:\\n        nan_count = df[col].isna().sum()\\n        pct = nan_count / rows_before * 100\\n        print(f\"  {col}: {nan_count:,} ({pct:.1f}%)\")\\n    \\n    # Remove rows\\n    df_clean = df.dropna(subset=cols_exist, how=how).reset_index(drop=True)\\n    df_clean = remove_nan_rows(\\n    df,\\n    cols=all_feature_cols,\\n    how=\\'any\\'\\n)\\n    \\n    rows_after = len(df_clean)\\n    rows_removed = rows_before - rows_after\\n    \\n    # Summary\\n    print()\\n    print(f\"Removal method: \\'{how}\\'\")\\n    print(\"-\" * 40)\\n    print(f\"Rows before: {rows_before:,}\")\\n    print(f\"Rows removed: {rows_removed:,} ({rows_removed / rows_before * 100:.1f}%)\")\\n    print(f\"Rows after: {rows_after:,}\")\\n    \\n    return df_clean'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def remove_nan_rows(df, cols, how='any'):\n",
    "    \"\"\"\n",
    "    Remove rows containing NaN in specified columns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The dataset\n",
    "    cols : list\n",
    "        List of column names to check for NaN\n",
    "    how : str, default 'any'\n",
    "        - 'any': Remove row if ANY of the specified columns has NaN\n",
    "        - 'all': Remove row only if ALL of the specified columns have NaN\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame with rows removed\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    # Remove rows where any of these columns is NaN\n",
    "    df_clean = remove_nan_rows(df, cols=['revt', 'at', 'ni'], how='any')\n",
    "    \n",
    "    # Remove rows only where ALL of these columns are NaN\n",
    "    df_clean = remove_nan_rows(df, cols=['revt', 'at', 'ni'], how='all')\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter to columns that exist\n",
    "    cols_exist = [c for c in cols if c in df.columns]\n",
    "    cols_missing = [c for c in cols if c not in df.columns]\n",
    "    \n",
    "    if cols_missing:\n",
    "        print(f\"Warning: Columns not found (ignored): {cols_missing}\")\n",
    "    \n",
    "    if not cols_exist:\n",
    "        print(\"No valid columns specified. Returning original DataFrame.\")\n",
    "        return df\n",
    "    \n",
    "    # Count NaN before removal\n",
    "    rows_before = len(df)\n",
    "    \n",
    "    # Per-column NaN counts\n",
    "    print(\"NaN counts per column before removal:\")\n",
    "    print(\"-\" * 40)\n",
    "    for col in cols_exist:\n",
    "        nan_count = df[col].isna().sum()\n",
    "        pct = nan_count / rows_before * 100\n",
    "        print(f\"  {col}: {nan_count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Remove rows\n",
    "    df_clean = df.dropna(subset=cols_exist, how=how).reset_index(drop=True)\n",
    "    df_clean = remove_nan_rows(\n",
    "    df,\n",
    "    cols=all_feature_cols,\n",
    "    how='any'\n",
    ")\n",
    "    \n",
    "    rows_after = len(df_clean)\n",
    "    rows_removed = rows_before - rows_after\n",
    "    \n",
    "    # Summary\n",
    "    print()\n",
    "    print(f\"Removal method: '{how}'\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Rows before: {rows_before:,}\")\n",
    "    print(f\"Rows removed: {rows_removed:,} ({rows_removed / rows_before * 100:.1f}%)\")\n",
    "    print(f\"Rows after: {rows_after:,}\")\n",
    "    \n",
    "    return df_clean'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a906c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan_rows(df, cols, how=\"any\"):\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    if not cols:\n",
    "        return df\n",
    "\n",
    "    if how == \"any\":\n",
    "        mask = df[cols].notna().all(axis=1)   # keep rows where ALL are present\n",
    "    elif how == \"all\":\n",
    "        mask = df[cols].notna().any(axis=1)   # keep rows where at least one is present\n",
    "    else:\n",
    "        raise ValueError(\"how must be 'any' or 'all'\")\n",
    "\n",
    "    return df.loc[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f592b1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "compustat_pre_cleaned = remove_nan_rows(compustat_pre_cleaned, all_feature_cols, how=\"any\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40a6168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_for_NaN_removal = all_feature_cols # make the choice of columns you want to use for NaN removal\n",
    "\n",
    "compustat_pre_cleaned = remove_nan_rows(\n",
    "    compustat_pre_cleaned,\n",
    "    cols_for_NaN_removal\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c01201c",
   "metadata": {},
   "source": [
    "# Logs, standardize, and train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafc7186",
   "metadata": {},
   "source": [
    "important: check which variables are standardized/logged in cell 21, and let me know if you think something should be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebcdae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signed_log_transform(df, cols):\n",
    "    \"\"\"Apply signed log transformation: sign(x) * log(1 + |x|)\"\"\"\n",
    "    df = df.copy()\n",
    "    cols_exist = [c for c in cols if c in df.columns]\n",
    "    for col in cols_exist:\n",
    "        df[col] = np.sign(df[col]) * np.log1p(np.abs(df[col]))\n",
    "    return df\n",
    "\n",
    "\n",
    "def standardize(df, cols, scaling_params=None):\n",
    "    \"\"\"\n",
    "    Standardize columns. If scaling_params provided, use those (for test data).\n",
    "    Otherwise compute from data (for training data) and return params.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    cols_exist = [c for c in cols if c in df.columns]\n",
    "    \n",
    "    if scaling_params is None:\n",
    "        # Training mode: compute parameters\n",
    "        scaling_params = {}\n",
    "        for col in cols_exist:\n",
    "            mean, std = df[col].mean(), df[col].std()\n",
    "            std = std if std != 0 else 1\n",
    "            df[col] = (df[col] - mean) / std\n",
    "            scaling_params[col] = {'mean': mean, 'std': std}\n",
    "        return df, scaling_params\n",
    "    else:\n",
    "        # Test mode: use provided parameters\n",
    "        for col in cols_exist:\n",
    "            if col in scaling_params:\n",
    "                mean = scaling_params[col]['mean']\n",
    "                std = scaling_params[col]['std']\n",
    "                df[col] = (df[col] - mean) / std\n",
    "        return df\n",
    "\n",
    "\n",
    "def prepare_data(df, feature_cols, target_col, log_cols, standardize_cols,\n",
    "                 test_size=0.2, random_state=12):\n",
    "    \"\"\"\n",
    "    Prepare data for SVM training.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "    feature_cols : list\n",
    "        All feature column names\n",
    "    target_col : str\n",
    "        Target variable name (e.g., 'default_next_period')\n",
    "    log_cols : list\n",
    "        Columns to apply signed log transform\n",
    "    standardize_cols : list\n",
    "        Columns to standardize\n",
    "    test_size : float\n",
    "        Proportion of data for test set\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (train_df, test_df, scaling_params)\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Split into train/test\n",
    "    train_df, test_df = train_test_split(\n",
    "        df, test_size=test_size, random_state=random_state,\n",
    "        stratify=df[target_col]\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {len(train_df):,}\")\n",
    "    print(f\"Test set: {len(test_df):,}\")\n",
    "    print(f\"Default rate (train): {train_df[target_col].mean():.2%}\")\n",
    "    print(f\"Default rate (test): {test_df[target_col].mean():.2%}\")\n",
    "    \n",
    "    # Apply log transform\n",
    "    train_df = signed_log_transform(train_df, log_cols)\n",
    "    test_df = signed_log_transform(test_df, log_cols)\n",
    "    \n",
    "    # Standardize (fit on train, apply to test)\n",
    "    train_df, scaling_params = standardize(train_df, standardize_cols)\n",
    "    test_df = standardize(test_df, standardize_cols, scaling_params)\n",
    "    \n",
    "    return train_df, test_df, scaling_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b31aaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_level_cols = ['revt', 'oibdp', 'ni', 'xint', 'dp', 'at', 'act', 'che',\n",
    "                  'lt', 'lct', 'dlc', 'dltt', 'ceq', 'csho', 'dv', 'total_debt',\n",
    "                  'cogs','ebit','re','rect','invt','oancf', 'capx', 'fincf', 'ivncf','prcc_f', 'market_capitalization']\n",
    "\n",
    "ratio_cols = ['gross_margin', 'net_profit_margin', 'roa', 'roe', 'asset_turnover',\n",
    "              'cash_to_assets', 'fixed_asset_intensity', 'current_ratio', 'quick_ratio',\n",
    "              'debt_to_assets', 'debt_to_equity', 'liabilities_to_assets',\n",
    "              'interest_coverage', 'long_term_debt_ratio', 'book_value_per_share',\n",
    "              'earnings_per_share', 'dividend_payout_ratio', 'dividend_yield', 'retention_ratio',\n",
    "              'capex_to_assets','investing_cash_flow_to_assets','days_sales_outstanding','days_inventory_outstanding',\n",
    "              'net_working_capital_to_assets','free_cash_flow_to_sales','financing_cash_flow_to_assets']\n",
    "\n",
    "growth_cols = ['revt_1_year_pct_change', 'at_1_year_pct_change', 'ni_1_year_pct_change',\n",
    "               'oibdp_1_year_pct_change', 'ceq_1_year_pct_change', 'total_debt_1_year_pct_change',\n",
    "               'revt_2_year_pct_change', 'at_2_year_pct_change', 'ni_2_year_pct_change',\n",
    "               'oibdp_2_year_pct_change', 'ceq_2_year_pct_change', 'total_debt_2_year_pct_change',\n",
    "               'revt_5_year_pct_change', 'at_5_year_pct_change', 'ni_5_year_pct_change',\n",
    "               'oibdp_5_year_pct_change', 'ceq_5_year_pct_change', 'total_debt_5_year_pct_change',\n",
    "               'capx_1_year_pct_change','market_capitalization_1_year_pct_change','free_cash_flow_to_sales_1_year_pct_change','financing_cash_flow_to_assets_1_year_pct_change',\n",
    "               'capx_2_year_pct_change','market_capitalization_2_year_pct_change','free_cash_flow_to_sales_2_year_pct_change','financing_cash_flow_to_assets_2_year_pct_change',\n",
    "               'capx_5_year_pct_change','market_capitalization_5_year_pct_change','free_cash_flow_to_sales_5_year_pct_change','financing_cash_flow_to_assets_5_year_pct_change'\n",
    "                ]\n",
    "\n",
    "feature_cols = raw_level_cols + ratio_cols + growth_cols\n",
    "log_cols = raw_level_cols  # Only log transform raw levels\n",
    "standardize_cols = feature_cols  # Standardize all features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27699811",
   "metadata": {},
   "source": [
    "chose what is your target columns (is it default in current period, in the t+1, t+2, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44739902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 145,139\n",
      "Test set: 36,285\n",
      "Default rate (train): 0.39%\n",
      "Default rate (test): 0.39%\n"
     ]
    }
   ],
   "source": [
    "target_col = 'default_next_1y' # choose the appropriate target variable\n",
    "\n",
    "train_set, test_set, scaling_params = prepare_data(compustat_pre_cleaned, feature_cols, target_col, log_cols, standardize_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7cba45",
   "metadata": {},
   "source": [
    "# Saving test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e49aa07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.to_csv(\n",
    "    \"train_set.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "test_set.to_csv(\n",
    "    \"test_set.csv\",\n",
    "    index=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
