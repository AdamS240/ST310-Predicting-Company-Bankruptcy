{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea0e0067",
   "metadata": {},
   "source": [
    "# Prepare the train and the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bd9c41",
   "metadata": {},
   "source": [
    "This code does the following:\n",
    "1. adds the nacis groupings \n",
    "2. adds default flags (default next year, default in 2 years, defaulted in the past, will ever default in the future)\n",
    "3. removes observations for which there was a default within the last 3 years\n",
    "4. removes NaN according to column selection\n",
    "5. logs and standardizes data\n",
    "\n",
    "Important parameters in section below:\n",
    "chosing which columns matter for NaN: \"cols_for_NaN_removal\"\n",
    "chosing target column (what will be your y value): \"target_col\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ed0d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62b5413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to the file obtained from cleandata.ipynb\n",
    "compustat_pre_cleaned_path = \"filepath_from_cleandata_notebook_output.csv\"\n",
    "\n",
    "compustat_pre_cleaned = pd.read_csv(compustat_pre_cleaned_path)\n",
    "compustat_pre_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5dc453",
   "metadata": {},
   "source": [
    "# Naics groupings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2fddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_naics_industry_group(df, naics_col='naicsh'):\n",
    "    \"\"\"\n",
    "    Add industry group classification based on 2-digit NAICS codes.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        Must contain the naics_col column\n",
    "    naics_col : str, default 'naicsh'\n",
    "        Column name containing NAICS codes\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame with added columns:\n",
    "        - naics_2_digit: First 2 digits of NAICS code\n",
    "        - naics_industry_group: Industry group classification string\n",
    "    \n",
    "    Industry Groups:\n",
    "    ----------------\n",
    "    - Capital Intensive: 11, 21, 22, 23, 31-33, 48-49\n",
    "    - Trade: 42, 44-45\n",
    "    - Financial and Real Estate: 52, 53\n",
    "    - Administrative: 54, 55, 56, 81, 92\n",
    "    - Healthcare and Education: 61, 62\n",
    "    - Food Services and Entertainment: 71, 72\n",
    "    - Other: All other codes\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Extract first 2 digits of NAICS code\n",
    "    # Handle float (e.g., 523100.0) by converting to int first, then string\n",
    "    df['naics_2_digit'] = (\n",
    "        df[naics_col]\n",
    "        .apply(lambda x: str(int(x))[:2] if pd.notna(x) else np.nan)\n",
    "    )\n",
    "    \n",
    "    # Convert to numeric for easier comparison\n",
    "    df['_naics_2_numeric'] = pd.to_numeric(df['naics_2_digit'], errors='coerce')\n",
    "    \n",
    "    # Define industry group mapping\n",
    "    def classify_industry(code):\n",
    "        if pd.isna(code):\n",
    "            return np.nan\n",
    "        \n",
    "        code = int(code)\n",
    "        \n",
    "        # Capital Intensive: 11, 21, 22, 23, 31-33, 48-49\n",
    "        if code in [11, 21, 22, 23] or (31 <= code <= 33) or (48 <= code <= 49):\n",
    "            return 'Capital Intensive'\n",
    "        \n",
    "        # Trade: 42, 44-45\n",
    "        elif code == 42 or (44 <= code <= 45):\n",
    "            return 'Trade'\n",
    "        \n",
    "        # Financial and Real Estate: 52, 53\n",
    "        elif code in [52, 53]:\n",
    "            return 'Financial and Real Estate'\n",
    "        \n",
    "        # Administrative: 54, 55, 56, 81, 92\n",
    "        elif code in [54, 55, 56, 81, 92]:\n",
    "            return 'Administrative'\n",
    "        \n",
    "        # Healthcare and Education: 61, 62\n",
    "        elif code in [61, 62]:\n",
    "            return 'Healthcare and Education'\n",
    "        \n",
    "        # Food Services and Entertainment: 71, 72\n",
    "        elif code in [71, 72]:\n",
    "            return 'Food Services and Entertainment'\n",
    "        \n",
    "        else:\n",
    "            return 'Other'\n",
    "    \n",
    "    df['naics_industry_group'] = df['_naics_2_numeric'].apply(classify_industry)\n",
    "    \n",
    "    # Drop helper column\n",
    "    df = df.drop(columns=['_naics_2_numeric'])\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"NAICS Industry Group Summary:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    total = len(df)\n",
    "    non_null = df['naics_industry_group'].notna().sum()\n",
    "    \n",
    "    print(f\"Total observations: {total:,}\")\n",
    "    print(f\"Observations with NAICS code: {non_null:,}\")\n",
    "    print(f\"Missing NAICS: {total - non_null:,}\")\n",
    "    print()\n",
    "    \n",
    "    group_counts = df['naics_industry_group'].value_counts(dropna=False)\n",
    "    print(\"Distribution by industry group:\")\n",
    "    for group, count in group_counts.items():\n",
    "        pct = count / total * 100\n",
    "        group_name = group if pd.notna(group) else 'Missing'\n",
    "        print(f\"  {group_name}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387a1979",
   "metadata": {},
   "outputs": [],
   "source": [
    "compustat_pre_cleaned = add_naics_industry_group(compustat_pre_cleaned, naics_col='naicsh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3de7617",
   "metadata": {},
   "source": [
    "# Default flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5ce439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_default_flags(df, default_indicator='TL_flag', default_value=1):\n",
    "    \"\"\"\n",
    "    Add multiple default-related columns to the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas DataFrame\n",
    "        Must contain columns: gvkey, fyear, and the default_indicator column\n",
    "    default_indicator : str, default 'TL_flag'\n",
    "        Column name that indicates default status (1 = default, 0 = no default)\n",
    "    default_value : int or float, default 1\n",
    "        Value in default_indicator that represents a default event\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas DataFrame with added columns:\n",
    "        - default_next_1y\n",
    "        - default_next_2y\n",
    "        - default_past_3y_plus\n",
    "        - default_ever_future\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(['gvkey', 'fyear']).reset_index(drop=True)\n",
    "\n",
    "    # ============================================================\n",
    "    # Helper: all default events\n",
    "    # ============================================================\n",
    "    defaults = (\n",
    "        df.loc[df[default_indicator] == default_value, ['gvkey', 'fyear']]\n",
    "        .rename(columns={'fyear': 'default_year'})\n",
    "    )\n",
    "\n",
    "    # ============================================================\n",
    "    # 1) DEFAULT IN NEXT 1 YEAR (fyear + 1)\n",
    "    # ============================================================\n",
    "    next_1y = (\n",
    "        df[['gvkey', 'fyear', default_indicator]]\n",
    "        .assign(fyear=lambda x: x['fyear'] - 1)\n",
    "        .rename(columns={default_indicator: 'default_next_1y'})\n",
    "    )\n",
    "\n",
    "    df = df.merge(next_1y, on=['gvkey', 'fyear'], how='left')\n",
    "\n",
    "    df['default_next_1y'] = np.where(\n",
    "        df['default_next_1y'].isna(),\n",
    "        np.nan,\n",
    "        (df['default_next_1y'] == default_value).astype(float)\n",
    "    )\n",
    "\n",
    "    # ============================================================\n",
    "    # 2) DEFAULT IN NEXT 2 YEARS (fyear + 1 OR fyear + 2)\n",
    "    # ============================================================\n",
    "    y1 = (\n",
    "        df[['gvkey', 'fyear', default_indicator]]\n",
    "        .assign(fyear=lambda x: x['fyear'] - 1)\n",
    "        .rename(columns={default_indicator: 'y1'})\n",
    "    )\n",
    "\n",
    "    y2 = (\n",
    "        df[['gvkey', 'fyear', default_indicator]]\n",
    "        .assign(fyear=lambda x: x['fyear'] - 2)\n",
    "        .rename(columns={default_indicator: 'y2'})\n",
    "    )\n",
    "\n",
    "    df = df.merge(y1, on=['gvkey', 'fyear'], how='left')\n",
    "    df = df.merge(y2, on=['gvkey', 'fyear'], how='left')\n",
    "\n",
    "    has_any_year = df[['y1', 'y2']].notna().any(axis=1)\n",
    "    has_default = (\n",
    "        (df['y1'] == default_value) |\n",
    "        (df['y2'] == default_value)\n",
    "    )\n",
    "\n",
    "    df['default_next_2y'] = np.where(\n",
    "        ~has_any_year,\n",
    "        np.nan,\n",
    "        has_default.astype(float)\n",
    "    )\n",
    "\n",
    "    df = df.drop(columns=['y1', 'y2'])\n",
    "\n",
    "    # ============================================================\n",
    "    # 3) DEFAULT MORE THAN 3 YEARS AGO (fyear - 4 or earlier)\n",
    "    # ============================================================\n",
    "    past = df.merge(defaults, on='gvkey', how='left')\n",
    "\n",
    "    past['past_default'] = (past['fyear'] - past['default_year']) > 3\n",
    "\n",
    "    past_flag = (\n",
    "        past.groupby(['gvkey', 'fyear'])['past_default']\n",
    "        .any()\n",
    "        .astype(float)\n",
    "        .reset_index(name='default_past_3y_plus')\n",
    "    )\n",
    "\n",
    "    df = df.merge(past_flag, on=['gvkey', 'fyear'], how='left')\n",
    "    df['default_past_3y_plus'] = df['default_past_3y_plus'].fillna(0.0)\n",
    "\n",
    "    # ============================================================\n",
    "    # 4) DEFAULT EVER IN FUTURE (any year > fyear)\n",
    "    # ============================================================\n",
    "    future = df[['gvkey', 'fyear']].merge(defaults, on='gvkey', how='left')\n",
    "\n",
    "    future['future_default'] = (future['default_year'] > future['fyear'])\n",
    "\n",
    "    future_flag = (\n",
    "        future.groupby(['gvkey', 'fyear'])['future_default']\n",
    "        .any()\n",
    "        .reset_index(name='default_ever_future')\n",
    "    )\n",
    "\n",
    "    df = df.merge(future_flag, on=['gvkey', 'fyear'], how='left')\n",
    "\n",
    "    # Identify whether future years exist (not just defaults)\n",
    "    max_fyear = df.groupby('gvkey')['fyear'].transform('max')\n",
    "\n",
    "    df['default_ever_future'] = np.where(\n",
    "        df['fyear'] == max_fyear,\n",
    "        np.nan,                      # no future years exist\n",
    "        df['default_ever_future'].astype(float)\n",
    "    )\n",
    "\n",
    "    # ============================================================\n",
    "    # SUMMARY (optional diagnostic output)\n",
    "    # ============================================================\n",
    "    print(\"Default Flags Summary:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for col in [\n",
    "        'default_next_1y',\n",
    "        'default_next_2y',\n",
    "        'default_past_3y_plus',\n",
    "        'default_ever_future'\n",
    "    ]:\n",
    "        non_null = df[col].notna().sum()\n",
    "        defaults_n = (df[col] == 1).sum()\n",
    "        rate = defaults_n / non_null * 100 if non_null > 0 else 0\n",
    "\n",
    "        print(f\"{col}:\")\n",
    "        print(f\"  Non-null observations: {non_null:,}\")\n",
    "        print(f\"  Defaults (=1): {defaults_n:,} ({rate:.2f}%)\\n\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457543d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "compustat_pre_cleaned = add_default_flags(compustat_pre_cleaned, default_indicator='TL_flag', default_value=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66b627c",
   "metadata": {},
   "source": [
    "# Removing periods between defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf06d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_post_default_observations(df, default_indicator='TL_flag', default_value=1, lookback_years=3):\n",
    "    \"\"\"\n",
    "    Remove observations if the firm had a default in the previous N years.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        Must contain columns: gvkey, fyear, and the default_indicator column\n",
    "    default_indicator : str, default 'TL_flag'\n",
    "        Column name that indicates default status\n",
    "    default_value : int/float, default 1\n",
    "        Value in default_indicator that represents a default event\n",
    "    lookback_years : int, default 3\n",
    "        Number of years to look back for prior defaults\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame with post-default observations removed\n",
    "    \n",
    "    Notes:\n",
    "    ------\n",
    "    - For each observation, checks if the firm had a default in any of the\n",
    "      previous `lookback_years` years (fyear-1 through fyear-lookback_years)\n",
    "    - Removes the observation if any prior default is found\n",
    "    - The default year itself is NOT removed (only subsequent years)\n",
    "    - Handles gaps in data correctly (only looks at years that exist)\n",
    "    \n",
    "    Example (lookback_years=5):\n",
    "    --------\n",
    "    If firm A has:\n",
    "        fyear=2018, TL_flag=0  -> KEEP (no prior default)\n",
    "        fyear=2019, TL_flag=1  -> KEEP (this is the default year)\n",
    "        fyear=2020, TL_flag=0  -> REMOVE (default in 2019, within 5 years)\n",
    "        fyear=2021, TL_flag=0  -> REMOVE (default in 2019, within 5 years)\n",
    "        fyear=2022, TL_flag=0  -> REMOVE (default in 2019, within 5 years)\n",
    "        fyear=2023, TL_flag=0  -> REMOVE (default in 2019, within 5 years)\n",
    "        fyear=2024, TL_flag=0  -> REMOVE (default in 2019, within 5 years)\n",
    "        fyear=2025, TL_flag=0  -> KEEP (2019 is now 6 years ago)\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Sort by company and fiscal year\n",
    "    df = df.sort_values(['gvkey', 'fyear']).reset_index(drop=True)\n",
    "    \n",
    "    # Get all default events\n",
    "    defaults = df[df[default_indicator] == default_value][['gvkey', 'fyear']].copy()\n",
    "    defaults = defaults.rename(columns={'fyear': 'default_year'})\n",
    "    \n",
    "    # For each observation, check if there's a default in the lookback window\n",
    "    # Merge all defaults for each company\n",
    "    df_with_defaults = df.merge(defaults, on='gvkey', how='left')\n",
    "    \n",
    "    # Calculate years since default\n",
    "    df_with_defaults['years_since_default'] = df_with_defaults['fyear'] - df_with_defaults['default_year']\n",
    "    \n",
    "    # Flag observations where a default occurred in the lookback window\n",
    "    # (years_since_default between 1 and lookback_years, inclusive)\n",
    "    df_with_defaults['has_recent_default'] = (\n",
    "        (df_with_defaults['years_since_default'] >= 1) & \n",
    "        (df_with_defaults['years_since_default'] <= lookback_years)\n",
    "    )\n",
    "    \n",
    "    # Group by original observation and check if ANY prior default exists\n",
    "    recent_default_flag = df_with_defaults.groupby(['gvkey', 'fyear'])['has_recent_default'].any().reset_index()\n",
    "    recent_default_flag = recent_default_flag.rename(columns={'has_recent_default': 'remove_flag'})\n",
    "    \n",
    "    # Merge back to original dataframe\n",
    "    df = df.merge(recent_default_flag, on=['gvkey', 'fyear'], how='left')\n",
    "    \n",
    "    # Fill NaN (firms with no defaults at all) with False\n",
    "    df['remove_flag'] = df['remove_flag'].fillna(False)\n",
    "    \n",
    "    # Print summary before removal\n",
    "    total_obs = len(df)\n",
    "    obs_to_remove = df['remove_flag'].sum()\n",
    "    firms_affected = df[df['remove_flag']]['gvkey'].nunique()\n",
    "    \n",
    "    print(\"Post-Default Observation Removal Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Lookback window: {lookback_years} years\")\n",
    "    print(f\"Total observations before: {total_obs:,}\")\n",
    "    print(f\"Observations to remove: {obs_to_remove:,}\")\n",
    "    print(f\"Firms affected: {firms_affected:,}\")\n",
    "    print(f\"Observations remaining: {total_obs - obs_to_remove:,}\")\n",
    "    print(f\"Percentage removed: {obs_to_remove / total_obs * 100:.2f}%\")\n",
    "    \n",
    "    # Remove flagged observations\n",
    "    df_clean = df[~df['remove_flag']].drop(columns=['remove_flag']).reset_index(drop=True)\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25a6d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "compustat_pre_cleaned = remove_post_default_observations(compustat_pre_cleaned, default_indicator='TL_flag', default_value=1, lookback_years=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06b00cc",
   "metadata": {},
   "source": [
    "# Remove NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a89730",
   "metadata": {},
   "source": [
    "Important: in cell 17, define which columns matter for you when removing NaN values. For example, if you do not care about missing values in columns like \"revt_5_year_pct_change\", do not include it in the list. \n",
    "\n",
    "The function remove_nan_rows takes in a list of columns as an argument, and removes all rows, which have a NaN entry in at least one of the specified columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3738c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan_rows(df, cols, how='any'):\n",
    "    \"\"\"\n",
    "    Remove rows containing NaN in specified columns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The dataset\n",
    "    cols : list\n",
    "        List of column names to check for NaN\n",
    "    how : str, default 'any'\n",
    "        - 'any': Remove row if ANY of the specified columns has NaN\n",
    "        - 'all': Remove row only if ALL of the specified columns have NaN\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame with rows removed\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    # Remove rows where any of these columns is NaN\n",
    "    df_clean = remove_nan_rows(df, cols=['revt', 'at', 'ni'], how='any')\n",
    "    \n",
    "    # Remove rows only where ALL of these columns are NaN\n",
    "    df_clean = remove_nan_rows(df, cols=['revt', 'at', 'ni'], how='all')\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter to columns that exist\n",
    "    cols_exist = [c for c in cols if c in df.columns]\n",
    "    cols_missing = [c for c in cols if c not in df.columns]\n",
    "    \n",
    "    if cols_missing:\n",
    "        print(f\"Warning: Columns not found (ignored): {cols_missing}\")\n",
    "    \n",
    "    if not cols_exist:\n",
    "        print(\"No valid columns specified. Returning original DataFrame.\")\n",
    "        return df\n",
    "    \n",
    "    # Count NaN before removal\n",
    "    rows_before = len(df)\n",
    "    \n",
    "    # Per-column NaN counts\n",
    "    print(\"NaN counts per column before removal:\")\n",
    "    print(\"-\" * 40)\n",
    "    for col in cols_exist:\n",
    "        nan_count = df[col].isna().sum()\n",
    "        pct = nan_count / rows_before * 100\n",
    "        print(f\"  {col}: {nan_count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Remove rows\n",
    "    df_clean = df.dropna(subset=cols_exist, how=how).reset_index(drop=True)\n",
    "    \n",
    "    rows_after = len(df_clean)\n",
    "    rows_removed = rows_before - rows_after\n",
    "    \n",
    "    # Summary\n",
    "    print()\n",
    "    print(f\"Removal method: '{how}'\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Rows before: {rows_before:,}\")\n",
    "    print(f\"Rows removed: {rows_removed:,} ({rows_removed / rows_before * 100:.1f}%)\")\n",
    "    print(f\"Rows after: {rows_after:,}\")\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a6168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_for_NaN_removal = [\n",
    "    'revt',    # Total Revenue\n",
    "    'at',      # Total Assets\n",
    "    'ni',      # Net Income\n",
    "    'lt',      # Total Liabilities\n",
    "    'ceq',     # Common Equity\n",
    "    'oancf'    # Operating Activities Cash Flow\n",
    "]\n",
    "\n",
    "compustat_pre_cleaned = remove_nan_rows(\n",
    "    compustat_pre_cleaned,\n",
    "    cols_for_NaN_removal\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c01201c",
   "metadata": {},
   "source": [
    "# Logs, standardize, and train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafc7186",
   "metadata": {},
   "source": [
    "important: check which variables are standardized/logged in cell 21, and let me know if you think something should be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcdae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signed_log_transform(df, cols):\n",
    "    \"\"\"Apply signed log transformation: sign(x) * log(1 + |x|)\"\"\"\n",
    "    df = df.copy()\n",
    "    cols_exist = [c for c in cols if c in df.columns]\n",
    "    for col in cols_exist:\n",
    "        df[col] = np.sign(df[col]) * np.log1p(np.abs(df[col]))\n",
    "    return df\n",
    "\n",
    "\n",
    "def standardize(df, cols, scaling_params=None):\n",
    "    \"\"\"\n",
    "    Standardize columns. If scaling_params provided, use those (for test data).\n",
    "    Otherwise compute from data (for training data) and return params.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    cols_exist = [c for c in cols if c in df.columns]\n",
    "    \n",
    "    if scaling_params is None:\n",
    "        # Training mode: compute parameters\n",
    "        scaling_params = {}\n",
    "        for col in cols_exist:\n",
    "            mean, std = df[col].mean(), df[col].std()\n",
    "            std = std if std != 0 else 1\n",
    "            df[col] = (df[col] - mean) / std\n",
    "            scaling_params[col] = {'mean': mean, 'std': std}\n",
    "        return df, scaling_params\n",
    "    else:\n",
    "        # Test mode: use provided parameters\n",
    "        for col in cols_exist:\n",
    "            if col in scaling_params:\n",
    "                mean = scaling_params[col]['mean']\n",
    "                std = scaling_params[col]['std']\n",
    "                df[col] = (df[col] - mean) / std\n",
    "        return df\n",
    "\n",
    "\n",
    "def prepare_data(df, feature_cols, target_col, log_cols, standardize_cols,\n",
    "                 test_size=0.2, random_state=12):\n",
    "    \"\"\"\n",
    "    Prepare data for SVM training.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "    feature_cols : list\n",
    "        All feature column names\n",
    "    target_col : str\n",
    "        Target variable name (e.g., 'default_next_period')\n",
    "    log_cols : list\n",
    "        Columns to apply signed log transform\n",
    "    standardize_cols : list\n",
    "        Columns to standardize\n",
    "    test_size : float\n",
    "        Proportion of data for test set\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (train_df, test_df, scaling_params)\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Split into train/test\n",
    "    train_df, test_df = train_test_split(\n",
    "        df, test_size=test_size, random_state=random_state,\n",
    "        stratify=df[target_col]\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {len(train_df):,}\")\n",
    "    print(f\"Test set: {len(test_df):,}\")\n",
    "    print(f\"Default rate (train): {train_df[target_col].mean():.2%}\")\n",
    "    print(f\"Default rate (test): {test_df[target_col].mean():.2%}\")\n",
    "    \n",
    "    # Apply log transform\n",
    "    train_df = signed_log_transform(train_df, log_cols)\n",
    "    test_df = signed_log_transform(test_df, log_cols)\n",
    "    \n",
    "    # Standardize (fit on train, apply to test)\n",
    "    train_df, scaling_params = standardize(train_df, standardize_cols)\n",
    "    test_df = standardize(test_df, standardize_cols, scaling_params)\n",
    "    \n",
    "    return train_df, test_df, scaling_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b31aaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_level_cols = ['revt', 'oibdp', 'ni', 'xint', 'dp', 'at', 'act', 'che',\n",
    "                  'lt', 'lct', 'dlc', 'dltt', 'ceq', 'csho', 'dv', 'total_debt',\n",
    "                  'cogs','ebit','re','rect','invt','oancf', 'capx', 'fincf', 'ivncf','prcc_f', 'market_capitalization']\n",
    "\n",
    "ratio_cols = ['gross_margin', 'net_profit_margin', 'roa', 'roe', 'asset_turnover',\n",
    "              'cash_to_assets', 'fixed_asset_intensity', 'current_ratio', 'quick_ratio',\n",
    "              'debt_to_assets', 'debt_to_equity', 'liabilities_to_assets',\n",
    "              'interest_coverage', 'long_term_debt_ratio', 'book_value_per_share',\n",
    "              'earnings_per_share', 'dividend_payout_ratio', 'dividend_yield', 'retention_ratio',\n",
    "              'capex_to_assets','investing_cash_flow_to_assets','days_sales_outstanding','days_inventory_outstanding',\n",
    "              'net_working_capital_to_assets','free_cash_flow_to_sales','financing_cash_flow_to_assets']\n",
    "\n",
    "growth_cols = ['revt_1_year_pct_change', 'at_1_year_pct_change', 'ni_1_year_pct_change',\n",
    "               'oibdp_1_year_pct_change', 'ceq_1_year_pct_change', 'total_debt_1_year_pct_change',\n",
    "               'revt_2_year_pct_change', 'at_2_year_pct_change', 'ni_2_year_pct_change',\n",
    "               'oibdp_2_year_pct_change', 'ceq_2_year_pct_change', 'total_debt_2_year_pct_change',\n",
    "               'revt_5_year_pct_change', 'at_5_year_pct_change', 'ni_5_year_pct_change',\n",
    "               'oibdp_5_year_pct_change', 'ceq_5_year_pct_change', 'total_debt_5_year_pct_change',\n",
    "               'capx_1_year_pct_change','market_capitalization_1_year_pct_change','free_cash_flow_to_sales_1_year_pct_change','financing_cash_flow_to_assets_1_year_pct_change',\n",
    "               'capx_2_year_pct_change','market_capitalization_2_year_pct_change','free_cash_flow_to_sales_2_year_pct_change','financing_cash_flow_to_assets_2_year_pct_change',\n",
    "               'capx_5_year_pct_change','market_capitalization_5_year_pct_change','free_cash_flow_to_sales_5_year_pct_change','financing_cash_flow_to_assets_5_year_pct_change'\n",
    "                ]\n",
    "\n",
    "feature_cols = raw_level_cols + ratio_cols + growth_cols\n",
    "log_cols = raw_level_cols  # Only log transform raw levels\n",
    "standardize_cols = feature_cols  # Standardize all features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27699811",
   "metadata": {},
   "source": [
    "chose what is your target columns (is it default in current period, in the t+1, t+2, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44739902",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'default_ever_future'\n",
    "\n",
    "train_set, test_set, scaling_params = prepare_data(compustat_pre_cleaned, feature_cols, target_col, log_cols, standardize_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7cba45",
   "metadata": {},
   "source": [
    "# Saving test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49aa07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.to_csv(\n",
    "    \"train_set.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "test_set.to_csv(\n",
    "    \"test_set.csv\",\n",
    "    index=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
